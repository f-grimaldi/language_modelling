{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from functools import reduce\n",
    "from torchvision import transforms\n",
    "from torch import optim, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.084141</td>\n",
       "      <td>0.050263</td>\n",
       "      <td>-0.083014</td>\n",
       "      <td>0.024498</td>\n",
       "      <td>0.069507</td>\n",
       "      <td>-0.008958</td>\n",
       "      <td>-0.100023</td>\n",
       "      <td>-0.035951</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>-0.132170</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060127</td>\n",
       "      <td>-0.031704</td>\n",
       "      <td>-0.069970</td>\n",
       "      <td>-0.009179</td>\n",
       "      <td>-0.089073</td>\n",
       "      <td>0.037803</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>-0.037058</td>\n",
       "      <td>-0.023169</td>\n",
       "      <td>-0.158178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.052883</td>\n",
       "      <td>-0.037741</td>\n",
       "      <td>0.091450</td>\n",
       "      <td>0.142502</td>\n",
       "      <td>0.106547</td>\n",
       "      <td>-0.095693</td>\n",
       "      <td>-0.124256</td>\n",
       "      <td>-0.081289</td>\n",
       "      <td>-0.053463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017933</td>\n",
       "      <td>0.140693</td>\n",
       "      <td>0.071709</td>\n",
       "      <td>-0.104425</td>\n",
       "      <td>0.050895</td>\n",
       "      <td>0.080463</td>\n",
       "      <td>-0.084460</td>\n",
       "      <td>-0.126534</td>\n",
       "      <td>0.009982</td>\n",
       "      <td>0.067867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.034092</td>\n",
       "      <td>0.067870</td>\n",
       "      <td>-0.037690</td>\n",
       "      <td>0.039759</td>\n",
       "      <td>0.071333</td>\n",
       "      <td>0.076393</td>\n",
       "      <td>-0.097779</td>\n",
       "      <td>-0.069916</td>\n",
       "      <td>-0.101197</td>\n",
       "      <td>-0.066318</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>0.015517</td>\n",
       "      <td>0.019767</td>\n",
       "      <td>-0.023118</td>\n",
       "      <td>-0.031326</td>\n",
       "      <td>0.050171</td>\n",
       "      <td>-0.018171</td>\n",
       "      <td>-0.080171</td>\n",
       "      <td>0.003688</td>\n",
       "      <td>0.022983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.142955</td>\n",
       "      <td>0.115191</td>\n",
       "      <td>-0.095155</td>\n",
       "      <td>0.036420</td>\n",
       "      <td>0.109865</td>\n",
       "      <td>0.146486</td>\n",
       "      <td>0.036642</td>\n",
       "      <td>-0.105708</td>\n",
       "      <td>0.020944</td>\n",
       "      <td>-0.035451</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070075</td>\n",
       "      <td>0.057465</td>\n",
       "      <td>0.015274</td>\n",
       "      <td>-0.012550</td>\n",
       "      <td>-0.078671</td>\n",
       "      <td>0.046206</td>\n",
       "      <td>-0.043623</td>\n",
       "      <td>-0.045520</td>\n",
       "      <td>-0.018946</td>\n",
       "      <td>-0.162184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.137284</td>\n",
       "      <td>-0.007928</td>\n",
       "      <td>0.060905</td>\n",
       "      <td>-0.035889</td>\n",
       "      <td>0.086667</td>\n",
       "      <td>0.006496</td>\n",
       "      <td>-0.083480</td>\n",
       "      <td>0.026690</td>\n",
       "      <td>-0.060219</td>\n",
       "      <td>-0.017208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019044</td>\n",
       "      <td>0.003692</td>\n",
       "      <td>0.042466</td>\n",
       "      <td>-0.006234</td>\n",
       "      <td>-0.039783</td>\n",
       "      <td>0.016603</td>\n",
       "      <td>-0.019024</td>\n",
       "      <td>-0.014788</td>\n",
       "      <td>-0.013053</td>\n",
       "      <td>-0.052533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0                                                                           \n",
       "the  0.084141  0.050263 -0.083014  0.024498  0.069507 -0.008958 -0.100023   \n",
       ",    0.002993  0.052883 -0.037741  0.091450  0.142502  0.106547 -0.095693   \n",
       ".    0.034092  0.067870 -0.037690  0.039759  0.071333  0.076393 -0.097779   \n",
       "of   0.142955  0.115191 -0.095155  0.036420  0.109865  0.146486  0.036642   \n",
       "to   0.137284 -0.007928  0.060905 -0.035889  0.086667  0.006496 -0.083480   \n",
       "\n",
       "           7         8         9     ...           40        41        42  \\\n",
       "0                                    ...                                    \n",
       "the -0.035951 -0.000141 -0.132170    ...    -0.060127 -0.031704 -0.069970   \n",
       ",   -0.124256 -0.081289 -0.053463    ...    -0.017933  0.140693  0.071709   \n",
       ".   -0.069916 -0.101197 -0.066318    ...    -0.000022  0.015517  0.019767   \n",
       "of  -0.105708  0.020944 -0.035451    ...    -0.070075  0.057465  0.015274   \n",
       "to   0.026690 -0.060219 -0.017208    ...    -0.019044  0.003692  0.042466   \n",
       "\n",
       "           43        44        45        46        47        48        49  \n",
       "0                                                                          \n",
       "the -0.009179 -0.089073  0.037803  0.000564 -0.037058 -0.023169 -0.158178  \n",
       ",   -0.104425  0.050895  0.080463 -0.084460 -0.126534  0.009982  0.067867  \n",
       ".   -0.023118 -0.031326  0.050171 -0.018171 -0.080171  0.003688  0.022983  \n",
       "of  -0.012550 -0.078671  0.046206 -0.043623 -0.045520 -0.018946 -0.162184  \n",
       "to  -0.006234 -0.039783  0.016603 -0.019024 -0.014788 -0.013053 -0.052533  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('lotr.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "text = text.lower()\n",
    "text = text.replace('#', '')\n",
    "text = text.replace('*', '')\n",
    "text = text.replace('(', '')\n",
    "text = text.replace(')', '')\n",
    "text = text.replace('`', \"'\")\n",
    "text = text.replace(')', '')\n",
    "text = text.replace('–', ' ')\n",
    "text = text.replace('-', ' ')\n",
    "text = text.replace('—', ' ')\n",
    "text = text.replace('»', '\"')\n",
    "text = text.replace('«', '\"')\n",
    "text = text.replace('_', ' ')\n",
    "text = text.replace('’', \"'\")\n",
    "text = text.replace('‘', \"'\")\n",
    "text = text.replace('ó', 'o')\n",
    "text = text.replace('{', '')\n",
    "text = text.replace('}', '')\n",
    "text = text.replace('µ', ' ')\n",
    "text = text.replace('¤', '')\n",
    "text = text.replace('¢', '')\n",
    "text = text.replace('¢', '')\n",
    "text = text.replace('®', '')\n",
    "text = text.replace('¥', '')\n",
    "text = text.replace('<br>', '')\n",
    "text = text.replace('<h4>', '')\n",
    "text = text.replace('</h4>', '')\n",
    "text = text.replace('/', '')\n",
    "text = text.replace('&', 'e')\n",
    "text = text.replace('=', 'o')\n",
    "text = text.replace('‚', ',')\n",
    "\n",
    "emb = pd.read_csv(r'glove.6B\\glove.6B.50d.txt', sep = ' ', quotechar=None, quoting=3, header=None)\n",
    "emb.index = emb.iloc[:, 0]\n",
    "emb.drop(columns=emb.columns[0], inplace=True)\n",
    "corpus = set(word for word in text.split())\n",
    "word_in_corpus = [i for i in emb.index if i in corpus]\n",
    "emb = emb.loc[word_in_corpus, :]\n",
    "emb = pd.DataFrame(np.round(emb.values, 4), index=emb.index)\n",
    "emb = emb.apply(lambda x: x/np.linalg.norm(x), axis=1)\n",
    "emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.to_csv(r'pre_trained_model\\\\embedding_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.to_json('glove_lotr.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'glove.6B\\\\glove.6B.200d.txt' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-83e9345175f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0memb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'glove.6B\\glove.6B.200d.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquotechar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0memb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0memb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mword_in_corpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0memb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'glove.6B\\\\glove.6B.200d.txt' does not exist"
     ]
    }
   ],
   "source": [
    "emb = pd.read_csv(r'glove.6B\\glove.6B.200d.txt', sep = ' ', quotechar=None, quoting=3, header=None)\n",
    "emb.index = emb.iloc[:, 0]\n",
    "emb.drop(columns=emb.columns[0], inplace=True)\n",
    "corpus = set(word for word in text.split())\n",
    "word_in_corpus = [i for i in emb.index if i in corpus]\n",
    "emb = emb.loc[word_in_corpus, :]\n",
    "emb = pd.DataFrame(np.round(emb.values, 4), index=emb.index)\n",
    "emb = emb.apply(lambda x: x/np.linalg.norm(x), axis=1)\n",
    "emb.to_json('glove_lotr_200d.json')\n",
    "emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "class LOTRDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, filepath, emb, transform=None):\n",
    "        \n",
    "        ### Load data\n",
    "        text = open(filepath, 'r', encoding='utf-8-sig').read()\n",
    "        \n",
    "        ### Preprocess data\n",
    "        # Remove the first and the last part (which are not sonnets in the text file)\n",
    "        # Lower case\n",
    "        text = text.lower()\n",
    "        text = text.replace('#', '')\n",
    "        text = text.replace('*', '')\n",
    "        text = text.replace('(', '')\n",
    "        text = text.replace(')', '')\n",
    "        text = text.replace('`', \"'\")\n",
    "        text = text.replace(')', '')\n",
    "        text = text.replace('–', ' ')\n",
    "        text = text.replace('-', ' ')\n",
    "        text = text.replace('—', ' ')\n",
    "        text = text.replace('»', '\"')\n",
    "        text = text.replace('«', '\"')\n",
    "        text = text.replace('_', ' ')\n",
    "        text = text.replace('’', \"'\")\n",
    "        text = text.replace('‘', \"'\")\n",
    "        text = text.replace('ó', 'o')\n",
    "        text = text.replace('{', '')\n",
    "        text = text.replace('}', '')\n",
    "        text = text.replace('µ', ' ')\n",
    "        text = text.replace('¤', '')\n",
    "        text = text.replace('¢', '')\n",
    "        text = text.replace('¢', '')\n",
    "        text = text.replace('®', '')\n",
    "        text = text.replace('¥', '')\n",
    "        text = text.replace('<br>', '')\n",
    "        text = text.replace('<h4>', '')\n",
    "        text = text.replace('</h4>', '')\n",
    "        text = text.replace('/', '')\n",
    "        text = text.replace('&', 'e')\n",
    "        text = text.replace('=', 'o')\n",
    "        text = text.replace('‚', ',')\n",
    "        \n",
    "\n",
    "        # Extract the sonnets (divided by empty lines and roman numerals)\n",
    "        sentences = re.split('[.]', text)\n",
    "        sentences = [i for i in sentences if len(i.split()) > 16]\n",
    "        chapter_list = sentences\n",
    "        ### Char to number\n",
    "        char_to_number = {key: value for key, value in zip(emb.index, emb.values)}\n",
    "                \n",
    "        ### Store data\n",
    "        self.corpus = text\n",
    "        self.chapter_list = chapter_list\n",
    "        self.transform = transform\n",
    "        self.emb = emb\n",
    "        self.char_to_number = char_to_number\n",
    "        #self.number_to_char = number_to_char\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.chapter_list)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Get sonnet text\n",
    "        text = self.chapter_list[idx]\n",
    "        \"\"\"\n",
    "        if len(text.split()) < 9:\n",
    "            print(self.chapter_list[idx])\n",
    "            print(text)\n",
    "        \"\"\"\n",
    "        # Encode with numbers\n",
    "        encoded = encode_text(self.char_to_number, text, self.emb)\n",
    "        # Create sample\n",
    "        sample = {'text': text, 'encoded': encoded}\n",
    "        # Transform (if defined)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "\n",
    "def encode_text(char_to_number, text, emb):\n",
    "    encoded = [char_to_number[c] for c in re.findall(r\"[\\w']+|[.,!?;]\", text) if c in emb.index]\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def decode_text(emb, encoded):\n",
    "    text = [emb.index[(emb == c).all(axis=1)][0] for c in encoded]\n",
    "    #text = [number_to_char[c] for c in encoded]\n",
    "    #text = reduce(lambda s1, s2: s1 + s2, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "class RandomCrop():\n",
    "    \n",
    "    def __init__(self, crop_len):\n",
    "        self.crop_len = crop_len\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        text = sample['text']\n",
    "        encoded = sample['encoded']\n",
    "        # Randomly choose an index\n",
    "        tot_words = len(text.split())\n",
    "        #start_words = np.random.randint(0, tot_words - self.crop_len)\n",
    "        start_words = 0\n",
    "        end_words = start_words + self.crop_len\n",
    "        new_text = ' '.join(text.split()[start_words:end_words])\n",
    "        #print(len(text.split()))\n",
    "        if len(new_text.split()) < self.crop_len:\n",
    "            print(len(new_text.split()))\n",
    "            print(new_text.split())\n",
    "        return {**sample,\n",
    "                'text': new_text,\n",
    "                'encoded': encoded[start_words: end_words]}\n",
    "    \n",
    "\n",
    "\n",
    "class OneHotEncoder():\n",
    "    \n",
    "    def __init__(self, emb=None):\n",
    "        self.emb = emb\n",
    "        #self.alphabet_len = alphabet_len\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        # Load encoded text with numbers\n",
    "        encoded = np.array(sample['encoded'])\n",
    "        # Create one hot matrix\n",
    "        #encoded_onehot = create_one_hot_matrix(encoded, self.alphabet_len)\n",
    "        encoded_onehot = encoded\n",
    "        return {**sample,\n",
    "                'encoded_onehot': encoded_onehot}\n",
    "        \n",
    "                \n",
    "class ToTensor():\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        # Convert one hot encoded text to pytorch tensor\n",
    "        encoded_onehot = torch.tensor(sample['encoded_onehot'])\n",
    "        return {'encoded_onehot': encoded_onehot}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == '__main__':\n",
    "#%% Initialize dataset\n",
    "filepath = 'lotr.txt'\n",
    "\n",
    "dataset = LOTRDataset(filepath, emb)\n",
    "\n",
    "#%% Test sampling\n",
    "sample = dataset[0]\n",
    "print('##############')\n",
    "print('##############')\n",
    "print('TEXT')\n",
    "print('##############')\n",
    "print(sample['text'])\n",
    "\n",
    "print('##############')\n",
    "print('##############')\n",
    "print('ENCODED')\n",
    "print('##############')\n",
    "print(sample['encoded'][0])\n",
    "\n",
    "#%% Test decode function\n",
    "encoded_text = sample['encoded'] #A list of np.array (,50)\n",
    "decoded_text = decode_text(dataset.emb, encoded_text)\n",
    "\n",
    "print('##############')\n",
    "print('##############')\n",
    "print('DECODED')\n",
    "print('##############')\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_len = 10\n",
    "rc = RandomCrop(crop_len)\n",
    "sample = rc(sample)\n",
    "print('Cropped text: ', sample['text'])\n",
    "print('Cropeed decoded text', decode_text(dataset.emb, sample['encoded']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Test OneHotEncoder\n",
    "#alphabet_len = len(dataset.alphabet)\n",
    "ohe = OneHotEncoder()\n",
    "sample = ohe(sample)\n",
    "print(sample['encoded_onehot'].shape)\n",
    "print('Cropped text: ', sample['text'])\n",
    "print('Cropeed decoded text', decode_text(dataset.emb, sample['encoded_onehot']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Test ToTensor\n",
    "print(sample['encoded'][0][0])\n",
    "tt = ToTensor()\n",
    "sampler = tt(sample)\n",
    "print(sampler['encoded_onehot'][0][0])\n",
    "print('Cropeed decoded text', decode_text(dataset.emb, sampler['encoded_onehot'].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Test dataloader\n",
    "filepath = 'lotr.txt'\n",
    "crop_len = 7\n",
    "#alphabet_len = len(dataset.alphabet)\n",
    "trans = transforms.Compose([RandomCrop(crop_len),\n",
    "                            OneHotEncoder(),\n",
    "                            ToTensor()\n",
    "                            ])\n",
    "dataset = LOTRDataset(filepath, emb, transform=trans)\n",
    "dataloader = DataLoader(dataset, batch_size=len(dataset.chapter_list)//80, shuffle=True)\n",
    "\n",
    "for batch_sample in dataloader:\n",
    "    batch_onehot = batch_sample['encoded_onehot']\n",
    "    print(batch_onehot.shape)\n",
    "    \n",
    "validation = batch_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = batch_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_units, layers_num, dropout_prob=0):\n",
    "        # Call the parent init function (required!)\n",
    "        super().__init__()\n",
    "        # Define recurrent layer\n",
    "        self.rnn = nn.LSTM(input_size=input_size, \n",
    "                           hidden_size=hidden_units,\n",
    "                           num_layers=layers_num,\n",
    "                           dropout=dropout_prob,\n",
    "                           batch_first=True)\n",
    "        # Define output layer\n",
    "        self.out = nn.Linear(hidden_units, input_size)\n",
    "        \n",
    "    def forward(self, x, state=None):\n",
    "        # LSTM\n",
    "        x, rnn_state = self.rnn(x, state)\n",
    "        # Linear layer\n",
    "        x = self.out(x)\n",
    "        return x, rnn_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(net, batch_onehot, loss_fn, optimizer):\n",
    "    \n",
    "    batch_onehot = batch_onehot.float()\n",
    "    ### Prepare network input and labels\n",
    "    # Get the labels (the last word of each sequence)\n",
    "    labels_onehot = batch_onehot[:, -1, :]\n",
    "    #labels_numbers = labels_onehot.argmax(dim=1)\n",
    "    # Remove the labels from the input tensor\n",
    "    net_input = batch_onehot[:, :-1, :]\n",
    "    # batch_onehot.shape =   [50, 100, 38]\n",
    "    # labels_onehot.shape =  [50, 38]\n",
    "    # labels_numbers.shape = [50]\n",
    "    # net_input.shape =      [50, 99, 38]\n",
    "    \n",
    "    ### Forward pass\n",
    "    # Eventually clear previous recorded gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    net_out, _ = net(net_input)\n",
    "    #net_out  = nn.functional.softmax(net_out, dim=1)\n",
    "    #print(net_out.shape)\n",
    "    ### Update network\n",
    "    #print('Shape of out and label: {}, {}'.format(net_out[:, -1, :].shape, labels_onehot.shape))\n",
    "    my_loss = torch.mean(torch.norm(net_out[:, -1, :] - labels_onehot, dim = 1))\n",
    "    \n",
    "    #print('My loss: {}'.format(my_loss))\n",
    "    #print('With shape: {}'.format(my_loss.shape))\n",
    "    # Evaluate loss only for last output\n",
    "    loss = loss_fn(net_out[:, -1, :], labels_onehot)\n",
    "    #print(\"It's loss: {}\".format(loss))\n",
    "    # Backward pass\n",
    "    #loss.backward(retain_graph=True)\n",
    "    loss.backward()\n",
    "    # Update\n",
    "    optimizer.step()\n",
    "    # Return average batch loss\n",
    "    return float(loss.data), my_loss, net_out, labels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customized_loss(y_true, y_pred):\n",
    "    loss = torch.mean(torch.norm(y_true-y_pred, dim=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(net, embedding_matrix, dataset, sentences='he went', length=30):\n",
    "    state = sentences\n",
    "    X = embedding_matrix.values\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Encode seed\n",
    "        seed_encoded = encode_text(dataset.char_to_number, state.lower(), embedding_matrix)\n",
    "        # One hot matrix\n",
    "        #seed_onehot = create_one_hot_matrix(seed_encoded, 47)\n",
    "        # To tensor\n",
    "        seed_onehot = torch.tensor(seed_encoded).float()\n",
    "        #print(seed_onehot)\n",
    "        # Add batch axis\n",
    "        seed_onehot = seed_onehot.unsqueeze(0)\n",
    "        # Forward pass\n",
    "        seed_onehot = seed_onehot.to(device)\n",
    "        net_out, net_state = net(seed_onehot)\n",
    "        # Get the most probable last output index\n",
    "        next_word_encoded = net_out[:, -1, :]\n",
    "        closest_value = np.linalg.norm((X - next_word_encoded.to('cpu').numpy()[0]), axis = 1)\n",
    "        closest_word = embedding_matrix.index[np.argmin(closest_value)]\n",
    "        state += ' ' + closest_word\n",
    "        # Print the seed letters\n",
    "        print(state, end=' ', flush=True)\n",
    "    #%% Generate sonnet\n",
    "    new_line_count = 0\n",
    "    tot_char_count = 0\n",
    "    while True:\n",
    "        with torch.no_grad(): # No need to track the gradients\n",
    "            # The new network input is the one hot encoding of the last chosen letter\n",
    "            net_input = encode_text(dataset.char_to_number, state.lower(), embedding_matrix)\n",
    "            net_input = torch.reshape(torch.tensor(net_input).float(), (1, -1, 50))\n",
    "            #print(net_input.shape)\n",
    "            #net_input = net_input.unsqueeze(0)\n",
    "            # Forward pass\n",
    "            net_input = net_input.to(device)\n",
    "            net_out, net_state = net(net_input, net_state)\n",
    "            \n",
    "            # Get the most probable letter index\n",
    "            np_out = net_out[:, -1, :].to('cpu').numpy()[0]\n",
    "            np_out = np_out/np.linalg.norm(np_out)\n",
    "            closest_value = np.linalg.norm((X - np_out), axis = 1)\n",
    "            closest_word = embedding_matrix.index[np.argmin(closest_value)]\n",
    "            \n",
    "            state += ' ' + closest_word\n",
    "            end = False\n",
    "            if closest_word == '.':\n",
    "                print(closest_word, end='\\n', flush=True)\n",
    "                end = True\n",
    "            else:\n",
    "                if end:\n",
    "                    print(closest_word.upper(), end=' ', flush=True)\n",
    "                else:\n",
    "                    print(closest_word, end=' ', flush=True)\n",
    "                end = False\n",
    "            # Count total letters\n",
    "            tot_char_count += 1\n",
    "            # Count new lines\n",
    "            # Break if 14 lines or 2000 letters\n",
    "            if tot_char_count > length:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-b3091e44cd2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mver\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\tAvarage lost in the last {}: {}\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mver\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mtesting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'does he'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#%% Check device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print('Selected device:', device)\n",
    "\n",
    "#%% Initialize network\n",
    "net = Network(input_size=50, \n",
    "              hidden_units=256, \n",
    "              layers_num=2, \n",
    "              dropout_prob=0.3)\n",
    "net.to(device)\n",
    "validation = validation.to(device).float()\n",
    "#%% Train network\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.003)\n",
    "# Define loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "tr_loss, val_loss = [], []\n",
    "ver = 20\n",
    "# Start training\n",
    "for epoch in range(600):\n",
    "    start = time.time()\n",
    "    if epoch%ver == 0:\n",
    "        if len(loss) > ver:\n",
    "            print('\\n\\tAvarage lost in the last {}: {}\\n'.format(ver, np.mean(loss[epoch-ver:epoch])))\n",
    "            testing(net, emb, dataset, 'does he', length = 50)\n",
    "        print('##################################')\n",
    "        print('## EPOCH %d' % (epoch + 1))\n",
    "        print('##################################')\n",
    "    b_losses, m_losses = [], []\n",
    "    # Iterate batches\n",
    "    for batch_sample in dataloader:\n",
    "        # Extract batch\n",
    "        batch_onehot = batch_sample['encoded_onehot'].to(device)\n",
    "        \n",
    "        # Update network\n",
    "        batch_loss, my_loss, out, y_true = train_batch(net, batch_onehot, loss_fn, optimizer)    \n",
    "        \n",
    "        b_losses.append(batch_loss)\n",
    "        m_losses.append(my_loss)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_validation = validation[:, -1, :]\n",
    "            #labels_numbers = labels_onehot.argmax(dim=1)\n",
    "            # Remove the labels from the input tensor\n",
    "            val_input = validation[:, :-1, :]\n",
    "            validation_pred, _ = net(val_input)\n",
    "            #net_out  = nn.functional.softmax(net_out, dim=1)\n",
    "            #print(net_out.shape)\n",
    "            ### Update network\n",
    "            # Evaluate loss only for last output\n",
    "            loss_val = loss_fn(validation_pred[:, -1, :], y_validation)\n",
    "        \n",
    "        #Clear cache\n",
    "        del batch_onehot\n",
    "        torch.cuda.empty_cache()\n",
    "        #if epoch%20 == 0:\n",
    "            #print('\\t Training loss (single batch):', batch_loss)\n",
    "    loss.append(torch.mean(torch.tensor(b_losses)))\n",
    "    #if epoch%20 == 0:\n",
    "    print('MSE loss: {}.'.format(torch.mean(torch.tensor(b_losses))), end='\\t')\n",
    "    print('Validation loss: {}'.format(loss_val.data), end='\\t')\n",
    "    print('Time: {}'.format(np.round(time.time()-start), 4))\n",
    "print(loss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(batch_out):\n",
    "    batch_size, nfeatures = batch_out.shape[0], batch_out.shape[-1] \n",
    "    batch = torch.reshape(batch_out, (-1, nfeatures))\n",
    "    batch = batch/torch.norm(batch, dim=1).view(batch.shape[0], 1)\n",
    "    return torch.reshape(batch_out, (batch_size, -1, nfeatures))\n",
    "\n",
    "out = normalize(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he went up and down the shire and went still well be as green as look some more well for only . his her him , as n't they heard daddy . just came just out . still apart in the twigs and coming up keep put with with flesh like they more instead well well him but but come did ? but but it the though but though but watching just instead cursed was pretended . even come only i him him some leaving to on when , they . but bouncing push the hand which one in the with of the demons took went came to but "
     ]
    }
   ],
   "source": [
    "state = 'he went up and down the shire'\n",
    "X = emb.values\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Encode seed\n",
    "    seed_encoded = encode_text(dataset.char_to_number, state.lower(), emb)\n",
    "    # One hot matrix\n",
    "    #seed_onehot = create_one_hot_matrix(seed_encoded, 47)\n",
    "    # To tensor\n",
    "    seed_onehot = torch.tensor(seed_encoded).float()\n",
    "    #print(seed_onehot)\n",
    "    # Add batch axis\n",
    "    seed_onehot = seed_onehot.unsqueeze(0)\n",
    "    # Forward pass\n",
    "    seed_onehot = seed_onehot.to(device)\n",
    "    net_out, net_state = net1(seed_onehot)\n",
    "    # Get the most probable last output index\n",
    "    next_word_encoded = net_out[:, -1, :]\n",
    "    closest_value = np.linalg.norm((X - next_word_encoded.to('cpu').numpy()[0]), axis = 1)\n",
    "    closest_word = emb.index[np.argmin(closest_value)]\n",
    "    state += ' ' + closest_word\n",
    "    # Print the seed letters\n",
    "    print(state, end=' ', flush=True)\n",
    "#%% Generate sonnet\n",
    "new_line_count = 0\n",
    "tot_char_count = 0\n",
    "while True:\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        # The new network input is the one hot encoding of the last chosen letter\n",
    "        net_input = encode_text(dataset.char_to_number, state.lower(), emb)\n",
    "        net_input = torch.reshape(torch.tensor(net_input).float(), (1, -1, 50))\n",
    "        #print(net_input.shape)\n",
    "        #net_input = net_input.unsqueeze(0)\n",
    "        # Forward pass\n",
    "        net_input = net_input.to(device)\n",
    "        net_out, net_state = net1(net_input, net_state)\n",
    "        \n",
    "        # Get the most probable letter index\n",
    "        closest_value = np.linalg.norm((X - net_out[:, -1, :].to('cpu').numpy()[0]), axis = 1)\n",
    "        closest_word = emb.index[np.argmin(closest_value)]\n",
    "        state += ' ' + closest_word\n",
    "        print(closest_word, end=' ', flush=True)\n",
    "        # Count total letters\n",
    "        tot_char_count += 1\n",
    "        # Count new lines\n",
    "        # Break if 14 lines or 2000 letters\n",
    "        if tot_char_count > 100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fgrim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Network. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(net, 'model_at_03_dec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(47, 512, num_layers=2, batch_first=True, dropout=0.1)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 47, got 50",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-92ac530642a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnet1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'model_long'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtesting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'he went up and down the shire'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-301cba79e39c>\u001b[0m in \u001b[0;36mtesting\u001b[1;34m(net, embedding_matrix, dataset, sentences, length)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mseed_onehot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseed_onehot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mnet_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed_onehot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Get the most probable last output index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mnext_word_encoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet_out\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-25a0e6d63d77>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, state)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# LSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Linear layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    562\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 564\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    565\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mGRU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNNBase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 543\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    544\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[1;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[0;32m    521\u001b[0m             \u001b[0mhx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m         \u001b[1;31m# type: (Tensor, Tuple[Tensor, Tensor], Optional[Tensor]) -> None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    147\u001b[0m             raise RuntimeError(\n\u001b[0;32m    148\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[1;32m--> 149\u001b[1;33m                     self.input_size, input.size(-1)))\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_expected_hidden_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 47, got 50"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "net1 = torch.load(r'model_long')\n",
    "print(net1.rnn)\n",
    "testing(net1, emb, dataset, 'he went up and down the shire', length = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'had', 'not', 'forgotten', 'the', 'look', 'of']\n",
      "['was', 'very', 'anxious', 'to', 'find', 'some', 'sign']\n",
      "['when', 'autumn', 'came', ',', 'he', 'knew', 'that']\n",
      "['i', 'imagine', 'what', 'spring', 'would', 'look', 'like']\n",
      "[\"'\", 'when', 'he', 'heard', 'what', 'frodo', 'had']\n",
      "torch.Size([5, 6, 50])\n",
      "##############\n",
      "had\twhen\n",
      "0.40604567527770996 \t 0.3945769998408468\n",
      "not\tturned\n",
      "0.8561952114105225 \t 0.43178995707111495\n",
      "forgotten\toccasion\n",
      "1.2295607328414917 \t 0.8516743971424025\n",
      "the\tconfirm\n",
      "1.3213441371917725 \t 0.9896899455375435\n",
      "look\tground\n",
      "1.0786402225494385 \t 0.8247855164957774\n",
      "of\tof\n",
      "0.03832031413912773 \t 0.038320315528186946\n",
      "##############\n",
      "very\tstill\n",
      "0.5128685235977173 \t 0.32169898089498167\n",
      "anxious\tmade\n",
      "0.9829562902450562 \t 0.5219056313426617\n",
      "to\t,\n",
      "0.5812139511108398 \t 0.23428700202510533\n",
      "find\tpity\n",
      "0.887028694152832 \t 0.7011574674714168\n",
      "some\thim\n",
      "0.7747485041618347 \t 0.33100224672743744\n",
      "sign\tsign\n",
      "0.46773993968963623 \t 0.4677399386228759\n",
      "##############\n",
      "autumn\tbut\n",
      "1.002932071685791 \t 0.24991816051544585\n",
      "came\t.\n",
      "0.4280683696269989 \t 0.39288504674658825\n",
      ",\tyellow\n",
      "0.9168917536735535 \t 0.6704107408373069\n",
      "he\tsoon\n",
      "0.7037444710731506 \t 0.5304748545729866\n",
      "knew\tspinning\n",
      "1.101285696029663 \t 0.8018807748640195\n",
      "that\tthat\n",
      "0.18727773427963257 \t 0.18727771731507825\n",
      "##############\n",
      "imagine\tbut\n",
      "0.6668714880943298 \t 0.23117118669066802\n",
      "what\twhich\n",
      "0.604468584060669 \t 0.3043024858804103\n",
      "spring\t.\n",
      "0.8221362829208374 \t 0.3991544976118416\n",
      "would\tthe\n",
      "0.830382227897644 \t 0.6160571096797925\n",
      "look\twant\n",
      "0.5638960599899292 \t 0.4453959593854115\n",
      "like\tlike\n",
      "0.22327923774719238 \t 0.22327924045556086\n",
      "##############\n",
      "when\tasked\n",
      "0.553896963596344 \t 0.4042290826960784\n",
      "he\tsaid\n",
      "0.8315635323524475 \t 0.4172607832950673\n",
      "heard\tcarried\n",
      "0.9739494323730469 \t 0.6981076052453312\n",
      "what\t,\n",
      "0.9541437029838562 \t 0.5221373788245569\n",
      "frodo\tquietly\n",
      "1.2218464612960815 \t 0.557297164593142\n",
      "had\thad\n",
      "0.19167090952396393 \t 0.19167091636710687\n"
     ]
    }
   ],
   "source": [
    "for batch_sample in dataloader:\n",
    "    # Extract batch\n",
    "    batch_onehot = batch_sample['encoded_onehot'][0:5, :, :]\n",
    "    for i in range(5):\n",
    "        print(decode_text(dataset.emb, batch_onehot[i, :, :].numpy()))\n",
    "    \n",
    "    batch_onehot_dev = batch_onehot.to(device)\n",
    "    # Update network\n",
    "    batch_loss, my_loss, out, y_true = train_batch(net, batch_onehot_dev, loss_fn, optimizer)\n",
    "    print(out.shape)\n",
    "    \n",
    "    values = []\n",
    "    for i in range(5):\n",
    "        print('##############')\n",
    "        out[i, :, :].shape\n",
    "        out_word = out[i, :, :].cpu().detach().numpy()\n",
    "        for j in range(6):\n",
    "            values.append(out_word[j])\n",
    "            #Letter to predict\n",
    "            true_word = decode_text(dataset.emb, batch_onehot[i, :, :].numpy())[j+1]\n",
    "            true_value = np.linalg.norm(batch_onehot[i, :, :].numpy()[j+1] - out_word[j])\n",
    "            true_value = float(torch.norm(batch_onehot[i, j+1, :].cpu().float() - out[i, j, :].cpu()).data)\n",
    "            print(true_word, end = '\\t')\n",
    "            closest_value = np.argmin(np.linalg.norm((X - out_word[j]), axis = 1))\n",
    "            #print(np.linalg.norm((X - out_word[j]), axis = 1).shape)\n",
    "            closest_word = emb.index[closest_value]\n",
    "            print(closest_word)\n",
    "            print(true_value, '\\t', np.min((np.linalg.norm((X - out_word[j]), axis = 1))))\n",
    "            #print(np.min((np.linalg.norm((X - out_word[j]), axis = 1)))-true_value)\n",
    "            \n",
    "            #print(emb[emb.index == ''])\n",
    "    \n",
    "    b_losses.append(batch_loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(values))\n",
    "df = pd.DataFrame(values)\n",
    "df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.loc['banners', :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

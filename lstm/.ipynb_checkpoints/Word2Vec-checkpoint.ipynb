{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from functools import reduce\n",
    "from torchvision import transforms\n",
    "from torch import optim, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.084141</td>\n",
       "      <td>0.050263</td>\n",
       "      <td>-0.083014</td>\n",
       "      <td>0.024498</td>\n",
       "      <td>0.069507</td>\n",
       "      <td>-0.008958</td>\n",
       "      <td>-0.100023</td>\n",
       "      <td>-0.035951</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>-0.132170</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060127</td>\n",
       "      <td>-0.031704</td>\n",
       "      <td>-0.069970</td>\n",
       "      <td>-0.009179</td>\n",
       "      <td>-0.089073</td>\n",
       "      <td>0.037803</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>-0.037058</td>\n",
       "      <td>-0.023169</td>\n",
       "      <td>-0.158178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.052883</td>\n",
       "      <td>-0.037741</td>\n",
       "      <td>0.091450</td>\n",
       "      <td>0.142502</td>\n",
       "      <td>0.106547</td>\n",
       "      <td>-0.095693</td>\n",
       "      <td>-0.124256</td>\n",
       "      <td>-0.081289</td>\n",
       "      <td>-0.053463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017933</td>\n",
       "      <td>0.140693</td>\n",
       "      <td>0.071709</td>\n",
       "      <td>-0.104425</td>\n",
       "      <td>0.050895</td>\n",
       "      <td>0.080463</td>\n",
       "      <td>-0.084460</td>\n",
       "      <td>-0.126534</td>\n",
       "      <td>0.009982</td>\n",
       "      <td>0.067867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.034092</td>\n",
       "      <td>0.067870</td>\n",
       "      <td>-0.037690</td>\n",
       "      <td>0.039759</td>\n",
       "      <td>0.071333</td>\n",
       "      <td>0.076393</td>\n",
       "      <td>-0.097779</td>\n",
       "      <td>-0.069916</td>\n",
       "      <td>-0.101197</td>\n",
       "      <td>-0.066318</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>0.015517</td>\n",
       "      <td>0.019767</td>\n",
       "      <td>-0.023118</td>\n",
       "      <td>-0.031326</td>\n",
       "      <td>0.050171</td>\n",
       "      <td>-0.018171</td>\n",
       "      <td>-0.080171</td>\n",
       "      <td>0.003688</td>\n",
       "      <td>0.022983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.142955</td>\n",
       "      <td>0.115191</td>\n",
       "      <td>-0.095155</td>\n",
       "      <td>0.036420</td>\n",
       "      <td>0.109865</td>\n",
       "      <td>0.146486</td>\n",
       "      <td>0.036642</td>\n",
       "      <td>-0.105708</td>\n",
       "      <td>0.020944</td>\n",
       "      <td>-0.035451</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070075</td>\n",
       "      <td>0.057465</td>\n",
       "      <td>0.015274</td>\n",
       "      <td>-0.012550</td>\n",
       "      <td>-0.078671</td>\n",
       "      <td>0.046206</td>\n",
       "      <td>-0.043623</td>\n",
       "      <td>-0.045520</td>\n",
       "      <td>-0.018946</td>\n",
       "      <td>-0.162184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.137284</td>\n",
       "      <td>-0.007928</td>\n",
       "      <td>0.060905</td>\n",
       "      <td>-0.035889</td>\n",
       "      <td>0.086667</td>\n",
       "      <td>0.006496</td>\n",
       "      <td>-0.083480</td>\n",
       "      <td>0.026690</td>\n",
       "      <td>-0.060219</td>\n",
       "      <td>-0.017208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019044</td>\n",
       "      <td>0.003692</td>\n",
       "      <td>0.042466</td>\n",
       "      <td>-0.006234</td>\n",
       "      <td>-0.039783</td>\n",
       "      <td>0.016603</td>\n",
       "      <td>-0.019024</td>\n",
       "      <td>-0.014788</td>\n",
       "      <td>-0.013053</td>\n",
       "      <td>-0.052533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0                                                                           \n",
       "the  0.084141  0.050263 -0.083014  0.024498  0.069507 -0.008958 -0.100023   \n",
       ",    0.002993  0.052883 -0.037741  0.091450  0.142502  0.106547 -0.095693   \n",
       ".    0.034092  0.067870 -0.037690  0.039759  0.071333  0.076393 -0.097779   \n",
       "of   0.142955  0.115191 -0.095155  0.036420  0.109865  0.146486  0.036642   \n",
       "to   0.137284 -0.007928  0.060905 -0.035889  0.086667  0.006496 -0.083480   \n",
       "\n",
       "           7         8         9     ...           40        41        42  \\\n",
       "0                                    ...                                    \n",
       "the -0.035951 -0.000141 -0.132170    ...    -0.060127 -0.031704 -0.069970   \n",
       ",   -0.124256 -0.081289 -0.053463    ...    -0.017933  0.140693  0.071709   \n",
       ".   -0.069916 -0.101197 -0.066318    ...    -0.000022  0.015517  0.019767   \n",
       "of  -0.105708  0.020944 -0.035451    ...    -0.070075  0.057465  0.015274   \n",
       "to   0.026690 -0.060219 -0.017208    ...    -0.019044  0.003692  0.042466   \n",
       "\n",
       "           43        44        45        46        47        48        49  \n",
       "0                                                                          \n",
       "the -0.009179 -0.089073  0.037803  0.000564 -0.037058 -0.023169 -0.158178  \n",
       ",   -0.104425  0.050895  0.080463 -0.084460 -0.126534  0.009982  0.067867  \n",
       ".   -0.023118 -0.031326  0.050171 -0.018171 -0.080171  0.003688  0.022983  \n",
       "of  -0.012550 -0.078671  0.046206 -0.043623 -0.045520 -0.018946 -0.162184  \n",
       "to  -0.006234 -0.039783  0.016603 -0.019024 -0.014788 -0.013053 -0.052533  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('lotr.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "text = text.lower()\n",
    "text = text.replace('#', '')\n",
    "text = text.replace('*', '')\n",
    "text = text.replace('(', '')\n",
    "text = text.replace(')', '')\n",
    "text = text.replace('`', \"'\")\n",
    "text = text.replace(')', '')\n",
    "text = text.replace('–', ' ')\n",
    "text = text.replace('-', ' ')\n",
    "text = text.replace('—', ' ')\n",
    "text = text.replace('»', '\"')\n",
    "text = text.replace('«', '\"')\n",
    "text = text.replace('_', ' ')\n",
    "text = text.replace('’', \"'\")\n",
    "text = text.replace('‘', \"'\")\n",
    "text = text.replace('ó', 'o')\n",
    "text = text.replace('{', '')\n",
    "text = text.replace('}', '')\n",
    "text = text.replace('µ', ' ')\n",
    "text = text.replace('¤', '')\n",
    "text = text.replace('¢', '')\n",
    "text = text.replace('¢', '')\n",
    "text = text.replace('®', '')\n",
    "text = text.replace('¥', '')\n",
    "text = text.replace('<br>', '')\n",
    "text = text.replace('<h4>', '')\n",
    "text = text.replace('</h4>', '')\n",
    "text = text.replace('/', '')\n",
    "text = text.replace('&', 'e')\n",
    "text = text.replace('=', 'o')\n",
    "text = text.replace('‚', ',')\n",
    "\n",
    "emb = pd.read_csv(r'glove.6B\\glove.6B.50d.txt', sep = ' ', quotechar=None, quoting=3, header=None)\n",
    "emb.index = emb.iloc[:, 0]\n",
    "emb.drop(columns=emb.columns[0], inplace=True)\n",
    "corpus = set(word for word in text.split())\n",
    "word_in_corpus = [i for i in emb.index if i in corpus]\n",
    "emb = emb.loc[word_in_corpus, :]\n",
    "emb = pd.DataFrame(np.round(emb.values, 4), index=emb.index)\n",
    "emb = emb.apply(lambda x: x/np.linalg.norm(x), axis=1)\n",
    "emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.to_csv(r'pre_trained_model\\\\embedding_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.to_json('glove_lotr.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.012710</td>\n",
       "      <td>0.016620</td>\n",
       "      <td>0.004213</td>\n",
       "      <td>-0.016051</td>\n",
       "      <td>0.009972</td>\n",
       "      <td>0.057859</td>\n",
       "      <td>-0.070747</td>\n",
       "      <td>-0.016371</td>\n",
       "      <td>0.010879</td>\n",
       "      <td>-0.033685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021651</td>\n",
       "      <td>0.035480</td>\n",
       "      <td>-0.036084</td>\n",
       "      <td>0.061272</td>\n",
       "      <td>-0.043248</td>\n",
       "      <td>0.023357</td>\n",
       "      <td>-0.001582</td>\n",
       "      <td>0.059761</td>\n",
       "      <td>0.005439</td>\n",
       "      <td>0.045470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>0.036286</td>\n",
       "      <td>0.060051</td>\n",
       "      <td>-0.000432</td>\n",
       "      <td>-0.077135</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.049299</td>\n",
       "      <td>-0.059393</td>\n",
       "      <td>-0.003002</td>\n",
       "      <td>-0.022594</td>\n",
       "      <td>0.032050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066979</td>\n",
       "      <td>0.039369</td>\n",
       "      <td>-0.031804</td>\n",
       "      <td>-0.030180</td>\n",
       "      <td>0.009662</td>\n",
       "      <td>0.006640</td>\n",
       "      <td>-0.045249</td>\n",
       "      <td>-0.042700</td>\n",
       "      <td>-0.047675</td>\n",
       "      <td>-0.022224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.025072</td>\n",
       "      <td>0.118405</td>\n",
       "      <td>-0.014199</td>\n",
       "      <td>-0.102594</td>\n",
       "      <td>0.021421</td>\n",
       "      <td>0.081500</td>\n",
       "      <td>-0.078828</td>\n",
       "      <td>-0.017198</td>\n",
       "      <td>0.024929</td>\n",
       "      <td>0.016382</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007181</td>\n",
       "      <td>0.036089</td>\n",
       "      <td>-0.010935</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>-0.006732</td>\n",
       "      <td>-0.016320</td>\n",
       "      <td>-0.049879</td>\n",
       "      <td>-0.007997</td>\n",
       "      <td>-0.033131</td>\n",
       "      <td>-0.019727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.009066</td>\n",
       "      <td>0.043583</td>\n",
       "      <td>0.053728</td>\n",
       "      <td>-0.061029</td>\n",
       "      <td>0.005073</td>\n",
       "      <td>0.087457</td>\n",
       "      <td>-0.018372</td>\n",
       "      <td>0.026050</td>\n",
       "      <td>0.009889</td>\n",
       "      <td>0.010540</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007010</td>\n",
       "      <td>0.066736</td>\n",
       "      <td>-0.018012</td>\n",
       "      <td>0.040052</td>\n",
       "      <td>0.016453</td>\n",
       "      <td>-0.051963</td>\n",
       "      <td>0.041972</td>\n",
       "      <td>-0.014790</td>\n",
       "      <td>-0.071844</td>\n",
       "      <td>0.079693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.093893</td>\n",
       "      <td>0.088687</td>\n",
       "      <td>-0.038441</td>\n",
       "      <td>-0.059332</td>\n",
       "      <td>0.066093</td>\n",
       "      <td>0.018648</td>\n",
       "      <td>-0.073559</td>\n",
       "      <td>-0.050737</td>\n",
       "      <td>-0.000868</td>\n",
       "      <td>0.095661</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045710</td>\n",
       "      <td>0.071611</td>\n",
       "      <td>-0.051146</td>\n",
       "      <td>0.021595</td>\n",
       "      <td>-0.054486</td>\n",
       "      <td>0.030910</td>\n",
       "      <td>-0.038343</td>\n",
       "      <td>0.089096</td>\n",
       "      <td>-0.037770</td>\n",
       "      <td>0.057220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0                                                                           \n",
       "the -0.012710  0.016620  0.004213 -0.016051  0.009972  0.057859 -0.070747   \n",
       ",    0.036286  0.060051 -0.000432 -0.077135  0.001007  0.049299 -0.059393   \n",
       ".    0.025072  0.118405 -0.014199 -0.102594  0.021421  0.081500 -0.078828   \n",
       "of   0.009066  0.043583  0.053728 -0.061029  0.005073  0.087457 -0.018372   \n",
       "to   0.093893  0.088687 -0.038441 -0.059332  0.066093  0.018648 -0.073559   \n",
       "\n",
       "          7         8         9      ...          190       191       192  \\\n",
       "0                                    ...                                    \n",
       "the -0.016371  0.010879 -0.033685    ...     0.021651  0.035480 -0.036084   \n",
       ",   -0.003002 -0.022594  0.032050    ...    -0.066979  0.039369 -0.031804   \n",
       ".   -0.017198  0.024929  0.016382    ...    -0.007181  0.036089 -0.010935   \n",
       "of   0.026050  0.009889  0.010540    ...    -0.007010  0.066736 -0.018012   \n",
       "to  -0.050737 -0.000868  0.095661    ...    -0.045710  0.071611 -0.051146   \n",
       "\n",
       "          193       194       195       196       197       198       199  \n",
       "0                                                                          \n",
       "the  0.061272 -0.043248  0.023357 -0.001582  0.059761  0.005439  0.045470  \n",
       ",   -0.030180  0.009662  0.006640 -0.045249 -0.042700 -0.047675 -0.022224  \n",
       ".    0.001428 -0.006732 -0.016320 -0.049879 -0.007997 -0.033131 -0.019727  \n",
       "of   0.040052  0.016453 -0.051963  0.041972 -0.014790 -0.071844  0.079693  \n",
       "to   0.021595 -0.054486  0.030910 -0.038343  0.089096 -0.037770  0.057220  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = pd.read_csv(r'glove.6B\\glove.6B.200d.txt', sep = ' ', quotechar=None, quoting=3, header=None)\n",
    "emb.index = emb.iloc[:, 0]\n",
    "emb.drop(columns=emb.columns[0], inplace=True)\n",
    "corpus = set(word for word in text.split())\n",
    "word_in_corpus = [i for i in emb.index if i in corpus]\n",
    "emb = emb.loc[word_in_corpus, :]\n",
    "emb = pd.DataFrame(np.round(emb.values, 4), index=emb.index)\n",
    "emb = emb.apply(lambda x: x/np.linalg.norm(x), axis=1)\n",
    "emb.to_json('glove_lotr_200d.json')\n",
    "emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "class LOTRDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, filepath, emb, transform=None):\n",
    "        \n",
    "        ### Load data\n",
    "        text = open(filepath, 'r', encoding='utf-8-sig').read()\n",
    "        \n",
    "        ### Preprocess data\n",
    "        # Remove the first and the last part (which are not sonnets in the text file)\n",
    "        # Lower case\n",
    "        text = text.lower()\n",
    "        text = text.replace('#', '')\n",
    "        text = text.replace('*', '')\n",
    "        text = text.replace('(', '')\n",
    "        text = text.replace(')', '')\n",
    "        text = text.replace('`', \"'\")\n",
    "        text = text.replace(')', '')\n",
    "        text = text.replace('–', ' ')\n",
    "        text = text.replace('-', ' ')\n",
    "        text = text.replace('—', ' ')\n",
    "        text = text.replace('»', '\"')\n",
    "        text = text.replace('«', '\"')\n",
    "        text = text.replace('_', ' ')\n",
    "        text = text.replace('’', \"'\")\n",
    "        text = text.replace('‘', \"'\")\n",
    "        text = text.replace('ó', 'o')\n",
    "        text = text.replace('{', '')\n",
    "        text = text.replace('}', '')\n",
    "        text = text.replace('µ', ' ')\n",
    "        text = text.replace('¤', '')\n",
    "        text = text.replace('¢', '')\n",
    "        text = text.replace('¢', '')\n",
    "        text = text.replace('®', '')\n",
    "        text = text.replace('¥', '')\n",
    "        text = text.replace('<br>', '')\n",
    "        text = text.replace('<h4>', '')\n",
    "        text = text.replace('</h4>', '')\n",
    "        text = text.replace('/', '')\n",
    "        text = text.replace('&', 'e')\n",
    "        text = text.replace('=', 'o')\n",
    "        text = text.replace('‚', ',')\n",
    "        \n",
    "\n",
    "        # Extract the sonnets (divided by empty lines and roman numerals)\n",
    "        sentences = re.split('[.]', text)\n",
    "        sentences = [i for i in sentences if len(i.split()) > 16]\n",
    "        chapter_list = sentences\n",
    "        ### Char to number\n",
    "        char_to_number = {key: value for key, value in zip(emb.index, emb.values)}\n",
    "                \n",
    "        ### Store data\n",
    "        self.corpus = text\n",
    "        self.chapter_list = chapter_list\n",
    "        self.transform = transform\n",
    "        self.emb = emb\n",
    "        self.char_to_number = char_to_number\n",
    "        #self.number_to_char = number_to_char\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.chapter_list)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Get sonnet text\n",
    "        text = self.chapter_list[idx]\n",
    "        \"\"\"\n",
    "        if len(text.split()) < 9:\n",
    "            print(self.chapter_list[idx])\n",
    "            print(text)\n",
    "        \"\"\"\n",
    "        # Encode with numbers\n",
    "        encoded = encode_text(self.char_to_number, text, self.emb)\n",
    "        # Create sample\n",
    "        sample = {'text': text, 'encoded': encoded}\n",
    "        # Transform (if defined)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "\n",
    "def encode_text(char_to_number, text, emb):\n",
    "    encoded = [char_to_number[c] for c in re.findall(r\"[\\w']+|[.,!?;]\", text) if c in emb.index]\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def decode_text(emb, encoded):\n",
    "    text = [emb.index[(emb == c).all(axis=1)][0] for c in encoded]\n",
    "    #text = [number_to_char[c] for c in encoded]\n",
    "    #text = reduce(lambda s1, s2: s1 + s2, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "class RandomCrop():\n",
    "    \n",
    "    def __init__(self, crop_len):\n",
    "        self.crop_len = crop_len\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        text = sample['text']\n",
    "        encoded = sample['encoded']\n",
    "        # Randomly choose an index\n",
    "        tot_words = len(text.split())\n",
    "        #start_words = np.random.randint(0, tot_words - self.crop_len)\n",
    "        start_words = 0\n",
    "        end_words = start_words + self.crop_len\n",
    "        new_text = ' '.join(text.split()[start_words:end_words])\n",
    "        #print(len(text.split()))\n",
    "        if len(new_text.split()) < self.crop_len:\n",
    "            print(len(new_text.split()))\n",
    "            print(new_text.split())\n",
    "        return {**sample,\n",
    "                'text': new_text,\n",
    "                'encoded': encoded[start_words: end_words]}\n",
    "    \n",
    "\n",
    "\n",
    "class OneHotEncoder():\n",
    "    \n",
    "    def __init__(self, emb=None):\n",
    "        self.emb = emb\n",
    "        #self.alphabet_len = alphabet_len\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        # Load encoded text with numbers\n",
    "        encoded = np.array(sample['encoded'])\n",
    "        # Create one hot matrix\n",
    "        #encoded_onehot = create_one_hot_matrix(encoded, self.alphabet_len)\n",
    "        encoded_onehot = encoded\n",
    "        return {**sample,\n",
    "                'encoded_onehot': encoded_onehot}\n",
    "        \n",
    "                \n",
    "class ToTensor():\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        # Convert one hot encoded text to pytorch tensor\n",
    "        encoded_onehot = torch.tensor(sample['encoded_onehot'])\n",
    "        return {'encoded_onehot': encoded_onehot}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "##############\n",
      "TEXT\n",
      "##############\n",
      " not a nasty, dirty, wet hole, filled with \n",
      "the ends of worms and an oozy smell, nor yet a dry, bare, sandy hole with nothing \n",
      "in it to sit down on or to eat: it was a hobbit hole, and that means comfort\n",
      "##############\n",
      "##############\n",
      "ENCODED\n",
      "##############\n",
      "[ 1.01619925e-01 -4.60632666e-02 -1.66226704e-04 -4.87598331e-02\n",
      "  1.09561867e-01  5.16226264e-02 -4.74115499e-02  1.71952290e-02\n",
      " -6.70263009e-02  1.67704275e-02  5.24722295e-02  1.31762367e-01\n",
      " -8.77492300e-02 -4.50843760e-02  1.63308502e-01  1.64582906e-01\n",
      "  7.94378948e-02 -5.04775091e-02  2.08337469e-02 -1.50823029e-01\n",
      " -7.62241785e-02  3.27835999e-02  1.14400911e-01  1.93377065e-02\n",
      "  6.15592893e-02 -4.27110281e-01 -9.67254720e-02 -4.04484979e-03\n",
      "  9.93666296e-02 -9.34932861e-02  7.14460843e-01  3.07334706e-02\n",
      " -1.32944424e-01 -1.38023573e-01  2.14801841e-02 -6.94088837e-02\n",
      "  1.02543407e-01  2.34194956e-02 -4.18152508e-02 -1.87836175e-02\n",
      " -6.54748517e-02  2.28099977e-02  3.05303046e-02  1.30063161e-01\n",
      " -1.48126463e-02 -1.26332295e-02 -1.24910133e-01  6.23534836e-02\n",
      "  9.25328651e-03  6.17993946e-02]\n",
      "##############\n",
      "##############\n",
      "DECODED\n",
      "##############\n",
      "['not', 'a', 'nasty', ',', 'dirty', ',', 'wet', 'hole', ',', 'filled', 'with', 'the', 'ends', 'of', 'worms', 'and', 'an', 'oozy', 'smell', ',', 'nor', 'yet', 'a', 'dry', ',', 'bare', ',', 'sandy', 'hole', 'with', 'nothing', 'in', 'it', 'to', 'sit', 'down', 'on', 'or', 'to', 'eat', 'it', 'was', 'a', 'hobbit', 'hole', ',', 'and', 'that', 'means', 'comfort']\n"
     ]
    }
   ],
   "source": [
    "#if __name__ == '__main__':\n",
    "#%% Initialize dataset\n",
    "filepath = 'lotr.txt'\n",
    "\n",
    "dataset = LOTRDataset(filepath, emb)\n",
    "\n",
    "#%% Test sampling\n",
    "sample = dataset[0]\n",
    "print('##############')\n",
    "print('##############')\n",
    "print('TEXT')\n",
    "print('##############')\n",
    "print(sample['text'])\n",
    "\n",
    "print('##############')\n",
    "print('##############')\n",
    "print('ENCODED')\n",
    "print('##############')\n",
    "print(sample['encoded'][0])\n",
    "\n",
    "#%% Test decode function\n",
    "encoded_text = sample['encoded'] #A list of np.array (,50)\n",
    "decoded_text = decode_text(dataset.emb, encoded_text)\n",
    "\n",
    "print('##############')\n",
    "print('##############')\n",
    "print('DECODED')\n",
    "print('##############')\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped text:  not a nasty, dirty, wet hole, filled with the ends\n",
      "Cropeed decoded text ['not', 'a', 'nasty', ',', 'dirty', ',', 'wet', 'hole', ',', 'filled']\n"
     ]
    }
   ],
   "source": [
    "crop_len = 10\n",
    "rc = RandomCrop(crop_len)\n",
    "sample = rc(sample)\n",
    "print('Cropped text: ', sample['text'])\n",
    "print('Cropeed decoded text', decode_text(dataset.emb, sample['encoded']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n",
      "Cropped text:  not a nasty, dirty, wet hole, filled with the ends\n",
      "Cropeed decoded text ['not', 'a', 'nasty', ',', 'dirty', ',', 'wet', 'hole', ',', 'filled']\n"
     ]
    }
   ],
   "source": [
    "#%% Test OneHotEncoder\n",
    "#alphabet_len = len(dataset.alphabet)\n",
    "ohe = OneHotEncoder()\n",
    "sample = ohe(sample)\n",
    "print(sample['encoded_onehot'].shape)\n",
    "print('Cropped text: ', sample['text'])\n",
    "print('Cropeed decoded text', decode_text(dataset.emb, sample['encoded_onehot']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10161992493469944\n",
      "tensor(0.1016, dtype=torch.float64)\n",
      "Cropeed decoded text ['not', 'a', 'nasty', ',', 'dirty', ',', 'wet', 'hole', ',', 'filled']\n"
     ]
    }
   ],
   "source": [
    "#%% Test ToTensor\n",
    "print(sample['encoded'][0][0])\n",
    "tt = ToTensor()\n",
    "sampler = tt(sample)\n",
    "print(sampler['encoded_onehot'][0][0])\n",
    "print('Cropeed decoded text', decode_text(dataset.emb, sampler['encoded_onehot'].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([178, 7, 50])\n",
      "torch.Size([56, 7, 50])\n"
     ]
    }
   ],
   "source": [
    "#%% Test dataloader\n",
    "\n",
    "crop_len = 7\n",
    "#alphabet_len = len(dataset.alphabet)\n",
    "trans = transforms.Compose([RandomCrop(crop_len),\n",
    "                            OneHotEncoder(),\n",
    "                            ToTensor()\n",
    "                            ])\n",
    "dataset = LOTRDataset(filepath, emb, transform=trans)\n",
    "dataloader = DataLoader(dataset, batch_size=len(dataset.chapter_list)//80, shuffle=True)\n",
    "\n",
    "for batch_sample in dataloader:\n",
    "    batch_onehot = batch_sample['encoded_onehot']\n",
    "    print(batch_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = batch_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_units, layers_num, dropout_prob=0):\n",
    "        # Call the parent init function (required!)\n",
    "        super().__init__()\n",
    "        # Define recurrent layer\n",
    "        self.rnn = nn.LSTM(input_size=input_size, \n",
    "                           hidden_size=hidden_units,\n",
    "                           num_layers=layers_num,\n",
    "                           dropout=dropout_prob,\n",
    "                           batch_first=True)\n",
    "        # Define output layer\n",
    "        self.out = nn.Linear(hidden_units, input_size)\n",
    "        \n",
    "    def forward(self, x, state=None):\n",
    "        # LSTM\n",
    "        x, rnn_state = self.rnn(x, state)\n",
    "        # Linear layer\n",
    "        x = self.out(x)\n",
    "        return x, rnn_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(net, batch_onehot, loss_fn, optimizer):\n",
    "    \n",
    "    batch_onehot = batch_onehot.float()\n",
    "    ### Prepare network input and labels\n",
    "    # Get the labels (the last word of each sequence)\n",
    "    labels_onehot = batch_onehot[:, -1, :]\n",
    "    #labels_numbers = labels_onehot.argmax(dim=1)\n",
    "    # Remove the labels from the input tensor\n",
    "    net_input = batch_onehot[:, :-1, :]\n",
    "    # batch_onehot.shape =   [50, 100, 38]\n",
    "    # labels_onehot.shape =  [50, 38]\n",
    "    # labels_numbers.shape = [50]\n",
    "    # net_input.shape =      [50, 99, 38]\n",
    "    \n",
    "    ### Forward pass\n",
    "    # Eventually clear previous recorded gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    net_out, _ = net(net_input)\n",
    "    #net_out  = nn.functional.softmax(net_out, dim=1)\n",
    "    #print(net_out.shape)\n",
    "    ### Update network\n",
    "    #print('Shape of out and label: {}, {}'.format(net_out[:, -1, :].shape, labels_onehot.shape))\n",
    "    my_loss = torch.mean(torch.norm(net_out[:, -1, :] - labels_onehot, dim = 1))\n",
    "    \n",
    "    #print('My loss: {}'.format(my_loss))\n",
    "    #print('With shape: {}'.format(my_loss.shape))\n",
    "    # Evaluate loss only for last output\n",
    "    loss = loss_fn(net_out[:, -1, :], labels_onehot)\n",
    "    #print(\"It's loss: {}\".format(loss))\n",
    "    # Backward pass\n",
    "    #loss.backward(retain_graph=True)\n",
    "    loss.backward()\n",
    "    # Update\n",
    "    optimizer.step()\n",
    "    # Return average batch loss\n",
    "    return float(loss.data), my_loss, net_out, labels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customized_loss(y_true, y_pred):\n",
    "    loss = torch.mean(torch.norm(y_true-y_pred, dim=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(net, embedding_matrix, dataset, sentences='he went', length=30):\n",
    "    state = sentences\n",
    "    X = embedding_matrix.values\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Encode seed\n",
    "        seed_encoded = encode_text(dataset.char_to_number, state.lower(), embedding_matrix)\n",
    "        # One hot matrix\n",
    "        #seed_onehot = create_one_hot_matrix(seed_encoded, 47)\n",
    "        # To tensor\n",
    "        seed_onehot = torch.tensor(seed_encoded).float()\n",
    "        #print(seed_onehot)\n",
    "        # Add batch axis\n",
    "        seed_onehot = seed_onehot.unsqueeze(0)\n",
    "        # Forward pass\n",
    "        seed_onehot = seed_onehot.to(device)\n",
    "        net_out, net_state = net(seed_onehot)\n",
    "        # Get the most probable last output index\n",
    "        next_word_encoded = net_out[:, -1, :]\n",
    "        closest_value = np.linalg.norm((X - next_word_encoded.to('cpu').numpy()[0]), axis = 1)\n",
    "        closest_word = embedding_matrix.index[np.argmin(closest_value)]\n",
    "        state += ' ' + closest_word\n",
    "        # Print the seed letters\n",
    "        print(state, end=' ', flush=True)\n",
    "    #%% Generate sonnet\n",
    "    new_line_count = 0\n",
    "    tot_char_count = 0\n",
    "    while True:\n",
    "        with torch.no_grad(): # No need to track the gradients\n",
    "            # The new network input is the one hot encoding of the last chosen letter\n",
    "            net_input = encode_text(dataset.char_to_number, state.lower(), embedding_matrix)\n",
    "            net_input = torch.reshape(torch.tensor(net_input).float(), (1, -1, 50))\n",
    "            #print(net_input.shape)\n",
    "            #net_input = net_input.unsqueeze(0)\n",
    "            # Forward pass\n",
    "            net_input = net_input.to(device)\n",
    "            net_out, net_state = net(net_input, net_state)\n",
    "            \n",
    "            # Get the most probable letter index\n",
    "            np_out = net_out[:, -1, :].to('cpu').numpy()[0]\n",
    "            np_out = np_out/np.linalg.norm(np_out)\n",
    "            closest_value = np.linalg.norm((X - np_out), axis = 1)\n",
    "            closest_word = embedding_matrix.index[np.argmin(closest_value)]\n",
    "            \n",
    "            state += ' ' + closest_word\n",
    "            end = False\n",
    "            if closest_word == '.':\n",
    "                print(closest_word, end='\\n', flush=True)\n",
    "                end = True\n",
    "            else:\n",
    "                if end:\n",
    "                    print(closest_word.upper(), end=' ', flush=True)\n",
    "                else:\n",
    "                    print(closest_word, end=' ', flush=True)\n",
    "                end = False\n",
    "            # Count total letters\n",
    "            tot_char_count += 1\n",
    "            # Count new lines\n",
    "            # Break if 14 lines or 2000 letters\n",
    "            if tot_char_count > length:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n",
      "##################################\n",
      "## EPOCH 1\n",
      "##################################\n",
      "MSE loss: 0.009580251760780811.\tMy loss: 0.6495471596717834.\tTime: 5.0\n",
      "MSE loss: 0.008978599682450294.\tMy loss: 0.6265156865119934.\tTime: 6.0\n",
      "MSE loss: 0.008879227563738823.\tMy loss: 0.6221598386764526.\tTime: 8.0\n",
      "MSE loss: 0.008836469613015652.\tMy loss: 0.6201225519180298.\tTime: 7.0\n",
      "MSE loss: 0.008798470720648766.\tMy loss: 0.619075357913971.\tTime: 5.0\n",
      "MSE loss: 0.008751085959374905.\tMy loss: 0.6167371273040771.\tTime: 5.0\n",
      "MSE loss: 0.008700728416442871.\tMy loss: 0.614556610584259.\tTime: 5.0\n",
      "MSE loss: 0.008659455925226212.\tMy loss: 0.6129568815231323.\tTime: 5.0\n",
      "MSE loss: 0.008609943091869354.\tMy loss: 0.6107795834541321.\tTime: 5.0\n",
      "MSE loss: 0.008586376905441284.\tMy loss: 0.6093582510948181.\tTime: 4.0\n",
      "MSE loss: 0.008537382818758488.\tMy loss: 0.6076706051826477.\tTime: 4.0\n",
      "MSE loss: 0.008490711450576782.\tMy loss: 0.6057952642440796.\tTime: 4.0\n",
      "MSE loss: 0.008452711626887321.\tMy loss: 0.6040715575218201.\tTime: 6.0\n",
      "MSE loss: 0.008422158658504486.\tMy loss: 0.6033080816268921.\tTime: 6.0\n",
      "MSE loss: 0.008360285311937332.\tMy loss: 0.6009446382522583.\tTime: 6.0\n",
      "MSE loss: 0.00827406719326973.\tMy loss: 0.597899317741394.\tTime: 5.0\n",
      "MSE loss: 0.008224481716752052.\tMy loss: 0.5959529280662537.\tTime: 4.0\n",
      "MSE loss: 0.008124321699142456.\tMy loss: 0.5922895669937134.\tTime: 4.0\n",
      "MSE loss: 0.008070074953138828.\tMy loss: 0.5904300808906555.\tTime: 4.0\n",
      "MSE loss: 0.007964392192661762.\tMy loss: 0.5865868330001831.\tTime: 4.0\n",
      "##################################\n",
      "## EPOCH 21\n",
      "##################################\n",
      "MSE loss: 0.007889644242823124.\tMy loss: 0.5839919447898865.\tTime: 4.0\n",
      "MSE loss: 0.007772199809551239.\tMy loss: 0.5795333981513977.\tTime: 5.0\n",
      "MSE loss: 0.007639001123607159.\tMy loss: 0.5747221112251282.\tTime: 4.0\n",
      "MSE loss: 0.007474151439964771.\tMy loss: 0.568688154220581.\tTime: 5.0\n",
      "MSE loss: 0.007352365180850029.\tMy loss: 0.5637831091880798.\tTime: 5.0\n",
      "MSE loss: 0.007239978294819593.\tMy loss: 0.5592403411865234.\tTime: 6.0\n",
      "MSE loss: 0.007064437493681908.\tMy loss: 0.5524954199790955.\tTime: 4.0\n",
      "MSE loss: 0.006929135415703058.\tMy loss: 0.5476227402687073.\tTime: 4.0\n",
      "MSE loss: 0.006778690032660961.\tMy loss: 0.5406680703163147.\tTime: 4.0\n",
      "MSE loss: 0.006619828753173351.\tMy loss: 0.5340952277183533.\tTime: 4.0\n",
      "MSE loss: 0.006465439219027758.\tMy loss: 0.5274358987808228.\tTime: 5.0\n",
      "MSE loss: 0.006354040000587702.\tMy loss: 0.5230520367622375.\tTime: 4.0\n",
      "MSE loss: 0.006185602862387896.\tMy loss: 0.5157384276390076.\tTime: 5.0\n",
      "MSE loss: 0.006054220721125603.\tMy loss: 0.5099717378616333.\tTime: 5.0\n",
      "MSE loss: 0.005917067173868418.\tMy loss: 0.5040193796157837.\tTime: 4.0\n",
      "MSE loss: 0.0057971663773059845.\tMy loss: 0.4989010989665985.\tTime: 4.0\n",
      "MSE loss: 0.005655284505337477.\tMy loss: 0.4924474060535431.\tTime: 4.0\n",
      "MSE loss: 0.00551301846280694.\tMy loss: 0.4864659309387207.\tTime: 4.0\n",
      "MSE loss: 0.005385477561503649.\tMy loss: 0.48061320185661316.\tTime: 4.0\n",
      "MSE loss: 0.0052548055537045.\tMy loss: 0.474047988653183.\tTime: 5.0\n",
      "\n",
      "\tAvarage lost in the last 20: 0.006567077245563269\n",
      "\n",
      "does he afterward somebody one really .\n",
      ".\n",
      "but but but when when when .\n",
      "seen .\n",
      "but when i not but but but so but but but but but but but but but but but but but but but but but but but but but but but but but but but but but ##################################\n",
      "## EPOCH 41\n",
      "##################################\n",
      "MSE loss: 0.005152800120413303.\tMy loss: 0.4697372019290924.\tTime: 5.0\n",
      "MSE loss: 0.005029173567891121.\tMy loss: 0.46402519941329956.\tTime: 5.0\n",
      "MSE loss: 0.004907487891614437.\tMy loss: 0.45860859751701355.\tTime: 5.0\n",
      "MSE loss: 0.004801630973815918.\tMy loss: 0.4536428153514862.\tTime: 5.0\n",
      "MSE loss: 0.004714442417025566.\tMy loss: 0.44946783781051636.\tTime: 5.0\n",
      "MSE loss: 0.004590771626681089.\tMy loss: 0.44343191385269165.\tTime: 5.0\n",
      "MSE loss: 0.004505983553826809.\tMy loss: 0.43921229243278503.\tTime: 4.0\n",
      "MSE loss: 0.004424846265465021.\tMy loss: 0.43524307012557983.\tTime: 5.0\n",
      "MSE loss: 0.004339553415775299.\tMy loss: 0.430543452501297.\tTime: 4.0\n",
      "MSE loss: 0.004278417211025953.\tMy loss: 0.42779871821403503.\tTime: 4.0\n",
      "MSE loss: 0.004175277426838875.\tMy loss: 0.4223201870918274.\tTime: 4.0\n",
      "MSE loss: 0.004111864138394594.\tMy loss: 0.4192335307598114.\tTime: 4.0\n",
      "MSE loss: 0.004040323663502932.\tMy loss: 0.4158053994178772.\tTime: 4.0\n",
      "MSE loss: 0.003973274026066065.\tMy loss: 0.4121766686439514.\tTime: 4.0\n",
      "MSE loss: 0.003913464490324259.\tMy loss: 0.40891170501708984.\tTime: 5.0\n",
      "MSE loss: 0.0038188386242836714.\tMy loss: 0.4040229320526123.\tTime: 5.0\n",
      "MSE loss: 0.0037781530991196632.\tMy loss: 0.4019578993320465.\tTime: 4.0\n",
      "MSE loss: 0.00374448555521667.\tMy loss: 0.39991360902786255.\tTime: 4.0\n",
      "MSE loss: 0.0036672346759587526.\tMy loss: 0.3960128128528595.\tTime: 4.0\n",
      "MSE loss: 0.003601629054173827.\tMy loss: 0.3921329081058502.\tTime: 4.0\n",
      "\n",
      "\tAvarage lost in the last 20: 0.004278481937944889\n",
      "\n",
      "does he knew mind the dark the .\n",
      "so me when frodo ! ' he one but but while the bridge of luni on the across of the foothills but but still but he picked hands , frodo .\n",
      "one but but what came to well came but seen just .\n",
      ".\n",
      "same same ##################################\n",
      "## EPOCH 61\n",
      "##################################\n",
      "MSE loss: 0.003527816152200103.\tMy loss: 0.3882916271686554.\tTime: 5.0\n",
      "MSE loss: 0.0034994445741176605.\tMy loss: 0.3865821063518524.\tTime: 4.0\n",
      "MSE loss: 0.0034493550192564726.\tMy loss: 0.38404393196105957.\tTime: 4.0\n",
      "MSE loss: 0.003416665131226182.\tMy loss: 0.381879985332489.\tTime: 4.0\n",
      "MSE loss: 0.0033750152215361595.\tMy loss: 0.3797028362751007.\tTime: 4.0\n",
      "MSE loss: 0.0033118166029453278.\tMy loss: 0.37620997428894043.\tTime: 4.0\n",
      "MSE loss: 0.003281791927292943.\tMy loss: 0.3742188811302185.\tTime: 4.0\n",
      "MSE loss: 0.0032555873040109873.\tMy loss: 0.37252819538116455.\tTime: 4.0\n",
      "MSE loss: 0.00320107932202518.\tMy loss: 0.3694796562194824.\tTime: 4.0\n",
      "MSE loss: 0.0031827264465391636.\tMy loss: 0.3682708144187927.\tTime: 4.0\n",
      "MSE loss: 0.003137084888294339.\tMy loss: 0.3656195104122162.\tTime: 4.0\n",
      "MSE loss: 0.0031054005958139896.\tMy loss: 0.3635655641555786.\tTime: 4.0\n",
      "MSE loss: 0.003066587494686246.\tMy loss: 0.36142244935035706.\tTime: 4.0\n",
      "MSE loss: 0.0030533429235219955.\tMy loss: 0.3607344925403595.\tTime: 4.0\n",
      "MSE loss: 0.0030257003381848335.\tMy loss: 0.35868895053863525.\tTime: 4.0\n",
      "MSE loss: 0.0029823288787156343.\tMy loss: 0.35621681809425354.\tTime: 4.0\n",
      "MSE loss: 0.002950779628008604.\tMy loss: 0.3542306125164032.\tTime: 4.0\n",
      "MSE loss: 0.0029182499274611473.\tMy loss: 0.3523496091365814.\tTime: 4.0\n",
      "MSE loss: 0.002913239412009716.\tMy loss: 0.35189288854599.\tTime: 4.0\n",
      "MSE loss: 0.0028920152690261602.\tMy loss: 0.35051432251930237.\tTime: 4.0\n",
      "\n",
      "\tAvarage lost in the last 20: 0.0031773014925420284\n",
      "\n",
      "does he felt trap .\n",
      "just .\n",
      ".\n",
      "when when , him why but something fact so that but as him having apparently turn and he never but looked as but they he but but they while but but often both always so but though even though though but somewhat suited because but he ##################################\n",
      "## EPOCH 81\n",
      "##################################\n",
      "MSE loss: 0.0028444172348827124.\tMy loss: 0.347818523645401.\tTime: 5.0\n",
      "MSE loss: 0.002805740339681506.\tMy loss: 0.3452697992324829.\tTime: 6.0\n",
      "MSE loss: 0.0027934457175433636.\tMy loss: 0.3444635272026062.\tTime: 5.0\n",
      "MSE loss: 0.0027752958703786135.\tMy loss: 0.3433651030063629.\tTime: 7.0\n",
      "MSE loss: 0.002754106419160962.\tMy loss: 0.34163549542427063.\tTime: 4.0\n",
      "MSE loss: 0.00274764490313828.\tMy loss: 0.34094303846359253.\tTime: 4.0\n",
      "MSE loss: 0.002711942419409752.\tMy loss: 0.33898666501045227.\tTime: 4.0\n",
      "MSE loss: 0.002704248297959566.\tMy loss: 0.338491290807724.\tTime: 4.0\n",
      "MSE loss: 0.00267592491582036.\tMy loss: 0.3365928828716278.\tTime: 4.0\n",
      "MSE loss: 0.0026579098775982857.\tMy loss: 0.3355138897895813.\tTime: 4.0\n",
      "MSE loss: 0.0026498273946344852.\tMy loss: 0.3348951041698456.\tTime: 4.0\n",
      "MSE loss: 0.002621015999466181.\tMy loss: 0.33268144726753235.\tTime: 4.0\n",
      "MSE loss: 0.002608656184747815.\tMy loss: 0.33202630281448364.\tTime: 4.0\n",
      "MSE loss: 0.002583121880888939.\tMy loss: 0.3300955593585968.\tTime: 5.0\n",
      "MSE loss: 0.0025897156447172165.\tMy loss: 0.33047741651535034.\tTime: 4.0\n",
      "MSE loss: 0.0025532017461955547.\tMy loss: 0.3284154236316681.\tTime: 5.0\n",
      "MSE loss: 0.0025249058380723.\tMy loss: 0.326138973236084.\tTime: 4.0\n",
      "MSE loss: 0.0025012691039592028.\tMy loss: 0.32511311769485474.\tTime: 4.0\n",
      "MSE loss: 0.002495347522199154.\tMy loss: 0.3241204619407654.\tTime: 5.0\n",
      "MSE loss: 0.002478558337315917.\tMy loss: 0.3229841887950897.\tTime: 4.0\n",
      "\n",
      "\tAvarage lost in the last 20: 0.0026538148522377014\n",
      "\n",
      "does he convinced you a sort kind of coming go come , and way .\n",
      "tell me .\n",
      "way ? ' , gandalf know what .\n",
      "but man him .\n",
      "but so but but but but but but even even but but having sounded not even more rather .\n",
      "even many tormenting not had ##################################\n",
      "## EPOCH 101\n",
      "##################################\n",
      "MSE loss: 0.0024659596383571625.\tMy loss: 0.3222649097442627.\tTime: 5.0\n",
      "MSE loss: 0.002456071088090539.\tMy loss: 0.32156652212142944.\tTime: 4.0\n",
      "MSE loss: 0.002433445304632187.\tMy loss: 0.3199613094329834.\tTime: 7.0\n",
      "MSE loss: 0.002425489481538534.\tMy loss: 0.31950870156288147.\tTime: 6.0\n",
      "MSE loss: 0.0023966175504028797.\tMy loss: 0.31738269329071045.\tTime: 4.0\n",
      "MSE loss: 0.002390979090705514.\tMy loss: 0.3167650103569031.\tTime: 4.0\n",
      "MSE loss: 0.0023759871255606413.\tMy loss: 0.31579989194869995.\tTime: 5.0\n",
      "MSE loss: 0.0023694445844739676.\tMy loss: 0.31562596559524536.\tTime: 5.0\n",
      "MSE loss: 0.0023598179686814547.\tMy loss: 0.314500629901886.\tTime: 4.0\n",
      "MSE loss: 0.0023531396873295307.\tMy loss: 0.3141954243183136.\tTime: 5.0\n",
      "MSE loss: 0.002342680236324668.\tMy loss: 0.31387409567832947.\tTime: 4.0\n",
      "MSE loss: 0.0023296198341995478.\tMy loss: 0.31289148330688477.\tTime: 4.0\n",
      "MSE loss: 0.0023054941557347775.\tMy loss: 0.31053605675697327.\tTime: 4.0\n",
      "MSE loss: 0.0023089139722287655.\tMy loss: 0.31065312027931213.\tTime: 4.0\n",
      "MSE loss: 0.002276479732245207.\tMy loss: 0.30848056077957153.\tTime: 5.0\n",
      "MSE loss: 0.002283412264660001.\tMy loss: 0.30928391218185425.\tTime: 5.0\n",
      "MSE loss: 0.0022542583756148815.\tMy loss: 0.3069647550582886.\tTime: 5.0\n",
      "MSE loss: 0.0022451570257544518.\tMy loss: 0.3063839375972748.\tTime: 4.0\n",
      "MSE loss: 0.002230787416920066.\tMy loss: 0.3054327964782715.\tTime: 4.0\n",
      "MSE loss: 0.0022364123724400997.\tMy loss: 0.3054262697696686.\tTime: 4.0\n",
      "\n",
      "\tAvarage lost in the last 20: 0.002342008287087083\n",
      "\n",
      "does he had screamed in the menacing inside ! they but you they were one well seeing well once suddenly up when but seen but came still on the on small small on instead well across the along across the along while bringing to the pool same looks kind that .\n",
      "so but if ##################################\n",
      "## EPOCH 121\n",
      "##################################\n",
      "MSE loss: 0.0022263173013925552.\tMy loss: 0.3047175705432892.\tTime: 5.0\n",
      "MSE loss: 0.0022001047618687153.\tMy loss: 0.3029000461101532.\tTime: 4.0\n",
      "MSE loss: 0.0022001639008522034.\tMy loss: 0.3028382658958435.\tTime: 4.0\n",
      "MSE loss: 0.002201634692028165.\tMy loss: 0.30296197533607483.\tTime: 4.0\n",
      "MSE loss: 0.0021910970099270344.\tMy loss: 0.30241987109184265.\tTime: 4.0\n",
      "MSE loss: 0.0021488508209586143.\tMy loss: 0.2995147407054901.\tTime: 4.0\n",
      "MSE loss: 0.002154148416593671.\tMy loss: 0.2993796169757843.\tTime: 4.0\n",
      "MSE loss: 0.0021579437889158726.\tMy loss: 0.29940277338027954.\tTime: 4.0\n",
      "MSE loss: 0.0021365012507885695.\tMy loss: 0.2981279790401459.\tTime: 4.0\n",
      "MSE loss: 0.0021289135329425335.\tMy loss: 0.2973126173019409.\tTime: 4.0\n",
      "MSE loss: 0.0021334418561309576.\tMy loss: 0.2977256774902344.\tTime: 4.0\n",
      "MSE loss: 0.002125701867043972.\tMy loss: 0.2971463203430176.\tTime: 4.0\n",
      "MSE loss: 0.0021138451993465424.\tMy loss: 0.29637768864631653.\tTime: 4.0\n",
      "MSE loss: 0.0020962355192750692.\tMy loss: 0.2948940098285675.\tTime: 4.0\n",
      "MSE loss: 0.002098330296576023.\tMy loss: 0.29486942291259766.\tTime: 4.0\n",
      "MSE loss: 0.002074279822409153.\tMy loss: 0.29333093762397766.\tTime: 4.0\n",
      "MSE loss: 0.002079957164824009.\tMy loss: 0.293785035610199.\tTime: 4.0\n",
      "MSE loss: 0.0020739592146128416.\tMy loss: 0.2931542694568634.\tTime: 4.0\n",
      "MSE loss: 0.002050739014521241.\tMy loss: 0.2915620505809784.\tTime: 4.0\n",
      "MSE loss: 0.002054597018286586.\tMy loss: 0.2915540039539337.\tTime: 5.0\n",
      "\n",
      "\tAvarage lost in the last 20: 0.0021323380060493946\n",
      "\n",
      "does he had whispered in the little him but he was out to but one but always saw but though took but few in the also healers .\n",
      ".\n",
      "rarely still when the saw upon that when when when once when one turned picked they went back .\n",
      "came out the bottoms and seen ##################################\n",
      "## EPOCH 141\n",
      "##################################\n",
      "MSE loss: 0.0020573213696479797.\tMy loss: 0.2916549742221832.\tTime: 6.0\n",
      "MSE loss: 0.00202993699349463.\tMy loss: 0.28981834650039673.\tTime: 5.0\n",
      "MSE loss: 0.0020290652755647898.\tMy loss: 0.28961309790611267.\tTime: 5.0\n",
      "MSE loss: 0.0020297567825764418.\tMy loss: 0.28928279876708984.\tTime: 5.0\n",
      "MSE loss: 0.0020262219477444887.\tMy loss: 0.2890131175518036.\tTime: 4.0\n",
      "MSE loss: 0.0019985868129879236.\tMy loss: 0.28718695044517517.\tTime: 4.0\n",
      "MSE loss: 0.001987677998840809.\tMy loss: 0.28651562333106995.\tTime: 4.0\n",
      "MSE loss: 0.0019810479134321213.\tMy loss: 0.28579989075660706.\tTime: 4.0\n",
      "MSE loss: 0.001980396918952465.\tMy loss: 0.28552114963531494.\tTime: 4.0\n",
      "MSE loss: 0.0019712592475116253.\tMy loss: 0.28488031029701233.\tTime: 4.0\n",
      "MSE loss: 0.0019596314523369074.\tMy loss: 0.28379708528518677.\tTime: 4.0\n",
      "MSE loss: 0.0019419386517256498.\tMy loss: 0.2830658257007599.\tTime: 4.0\n",
      "MSE loss: 0.0019600624218583107.\tMy loss: 0.2839662432670593.\tTime: 4.0\n",
      "MSE loss: 0.0019375490956008434.\tMy loss: 0.28189733624458313.\tTime: 4.0\n",
      "MSE loss: 0.0019411866087466478.\tMy loss: 0.2825504243373871.\tTime: 4.0\n",
      "MSE loss: 0.0019169696606695652.\tMy loss: 0.2802824079990387.\tTime: 4.0\n",
      "MSE loss: 0.0019287881441414356.\tMy loss: 0.2813873887062073.\tTime: 4.0\n",
      "MSE loss: 0.0019156263442710042.\tMy loss: 0.28011852502822876.\tTime: 4.0\n",
      "MSE loss: 0.0019162676762789488.\tMy loss: 0.2799786329269409.\tTime: 5.0\n",
      "MSE loss: 0.0019090892747044563.\tMy loss: 0.2800244688987732.\tTime: 4.0\n",
      "\n",
      "\tAvarage lost in the last 20: 0.0019709188491106033\n",
      "\n",
      "does he had coughed in nose turned hand put while .\n",
      ".\n",
      "the carved on along and turning into into through and one in hills and town .\n",
      "where coming , not replied n't that never i had but had even the still still few where home times of the west the the black ##################################\n",
      "## EPOCH 161\n",
      "##################################\n",
      "MSE loss: 0.0019091909052804112.\tMy loss: 0.27975067496299744.\tTime: 5.0\n",
      "MSE loss: 0.0018991612596437335.\tMy loss: 0.27876347303390503.\tTime: 5.0\n",
      "MSE loss: 0.0018981745233759284.\tMy loss: 0.278756320476532.\tTime: 5.0\n",
      "MSE loss: 0.0018830898916348815.\tMy loss: 0.277592271566391.\tTime: 6.0\n",
      "MSE loss: 0.0018789564492180943.\tMy loss: 0.27727648615837097.\tTime: 5.0\n",
      "MSE loss: 0.001875061891041696.\tMy loss: 0.276795893907547.\tTime: 5.0\n",
      "MSE loss: 0.0018752766773104668.\tMy loss: 0.27707594633102417.\tTime: 4.0\n",
      "MSE loss: 0.0018573137931525707.\tMy loss: 0.27554231882095337.\tTime: 4.0\n",
      "MSE loss: 0.001859693438746035.\tMy loss: 0.2755448520183563.\tTime: 4.0\n",
      "MSE loss: 0.001843456644564867.\tMy loss: 0.274480402469635.\tTime: 4.0\n",
      "MSE loss: 0.0018391048070043325.\tMy loss: 0.2739320993423462.\tTime: 5.0\n",
      "MSE loss: 0.0018461793661117554.\tMy loss: 0.27448099851608276.\tTime: 4.0\n",
      "MSE loss: 0.0018349464517086744.\tMy loss: 0.273503839969635.\tTime: 4.0\n",
      "MSE loss: 0.0018351668259128928.\tMy loss: 0.2736968398094177.\tTime: 4.0\n",
      "MSE loss: 0.001821385812945664.\tMy loss: 0.27204224467277527.\tTime: 4.0\n",
      "MSE loss: 0.0018071082886308432.\tMy loss: 0.2713496685028076.\tTime: 4.0\n",
      "MSE loss: 0.0018120823660865426.\tMy loss: 0.2713490128517151.\tTime: 4.0\n",
      "MSE loss: 0.001824773382395506.\tMy loss: 0.2727077007293701.\tTime: 5.0\n",
      "MSE loss: 0.0018065032782033086.\tMy loss: 0.2710943818092346.\tTime: 4.0\n",
      "MSE loss: 0.0018022669246420264.\tMy loss: 0.2707841098308563.\tTime: 4.0\n",
      "\n",
      "\tAvarage lost in the last 20: 0.0018504445906728506\n",
      "\n",
      "does he when demons this sauron .\n",
      "what so as kind like a sounds most up one up the west where the outer where .\n",
      "descend here which the new where on turning back while more the along between the along still still now well ugly and turned instead but long when came again ##################################\n",
      "## EPOCH 181\n",
      "##################################\n",
      "MSE loss: 0.001792802126146853.\tMy loss: 0.26978492736816406.\tTime: 5.0\n",
      "MSE loss: 0.0017859525978565216.\tMy loss: 0.2692836821079254.\tTime: 5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE loss: 0.0017855928745120764.\tMy loss: 0.2692418694496155.\tTime: 4.0\n",
      "MSE loss: 0.0017850995063781738.\tMy loss: 0.269369512796402.\tTime: 4.0\n",
      "MSE loss: 0.0017770662670955062.\tMy loss: 0.2685282528400421.\tTime: 4.0\n",
      "MSE loss: 0.0017791821155697107.\tMy loss: 0.2683422565460205.\tTime: 4.0\n",
      "MSE loss: 0.001763261971063912.\tMy loss: 0.2672933340072632.\tTime: 5.0\n",
      "MSE loss: 0.0017655898118391633.\tMy loss: 0.26725441217422485.\tTime: 5.0\n",
      "MSE loss: 0.0017608562484383583.\tMy loss: 0.2665719985961914.\tTime: 4.0\n",
      "MSE loss: 0.0017432956956326962.\tMy loss: 0.2656269967556.\tTime: 4.0\n",
      "MSE loss: 0.0017411325825378299.\tMy loss: 0.26586228609085083.\tTime: 4.0\n",
      "MSE loss: 0.0017385907704010606.\tMy loss: 0.2653450667858124.\tTime: 4.0\n",
      "MSE loss: 0.0017223700415343046.\tMy loss: 0.2638152837753296.\tTime: 4.0\n",
      "MSE loss: 0.0017326300730928779.\tMy loss: 0.264523983001709.\tTime: 4.0\n",
      "MSE loss: 0.001728986855596304.\tMy loss: 0.26435962319374084.\tTime: 4.0\n",
      "MSE loss: 0.0017235117265954614.\tMy loss: 0.2634815275669098.\tTime: 4.0\n",
      "MSE loss: 0.0017046763096004725.\tMy loss: 0.2621299624443054.\tTime: 4.0\n",
      "MSE loss: 0.001714575570076704.\tMy loss: 0.2628929615020752.\tTime: 4.0\n",
      "MSE loss: 0.0017088952008634806.\tMy loss: 0.26221057772636414.\tTime: 4.0\n",
      "MSE loss: 0.001690912526100874.\tMy loss: 0.2610745429992676.\tTime: 4.0\n",
      "\n",
      "\tAvarage lost in the last 20: 0.0017472489271312952\n",
      "\n",
      "does he knew somehow to again in the along moving up to the road .\n",
      "the filled but but watching and cheering sometimes well even seemed .\n",
      "but but restless .\n",
      "the beneath thrown , little but even so in the much surface though though though .\n",
      "even my once so he he calmly ##################################\n",
      "## EPOCH 201\n",
      "##################################\n",
      "MSE loss: 0.0017040679231286049.\tMy loss: 0.26213809847831726.\tTime: 5.0\n",
      "MSE loss: 0.0016888728132471442.\tMy loss: 0.2609143555164337.\tTime: 6.0\n",
      "MSE loss: 0.0016918191686272621.\tMy loss: 0.26103121042251587.\tTime: 4.0\n",
      "MSE loss: 0.0016834842972457409.\tMy loss: 0.26006531715393066.\tTime: 4.0\n",
      "MSE loss: 0.0016887716483324766.\tMy loss: 0.2608836889266968.\tTime: 4.0\n",
      "MSE loss: 0.0016827441286295652.\tMy loss: 0.26023969054222107.\tTime: 4.0\n",
      "MSE loss: 0.0016755860997363925.\tMy loss: 0.2593189775943756.\tTime: 4.0\n",
      "MSE loss: 0.0016814994160085917.\tMy loss: 0.2595836818218231.\tTime: 4.0\n",
      "MSE loss: 0.0016795825213193893.\tMy loss: 0.25954392552375793.\tTime: 4.0\n",
      "MSE loss: 0.0016670369077473879.\tMy loss: 0.2586926519870758.\tTime: 4.0\n",
      "MSE loss: 0.001661605667322874.\tMy loss: 0.25796735286712646.\tTime: 4.0\n",
      "MSE loss: 0.0016519819619134068.\tMy loss: 0.2574837803840637.\tTime: 4.0\n",
      "MSE loss: 0.0016588346334174275.\tMy loss: 0.2578238248825073.\tTime: 4.0\n",
      "MSE loss: 0.00164586934261024.\tMy loss: 0.2567065954208374.\tTime: 4.0\n",
      "MSE loss: 0.0016411177348345518.\tMy loss: 0.2564755082130432.\tTime: 4.0\n",
      "MSE loss: 0.0016422204207628965.\tMy loss: 0.25630974769592285.\tTime: 4.0\n",
      "MSE loss: 0.001634262385778129.\tMy loss: 0.2556566298007965.\tTime: 4.0\n",
      "MSE loss: 0.0016338266432285309.\tMy loss: 0.255933940410614.\tTime: 4.0\n",
      "MSE loss: 0.0016354246763512492.\tMy loss: 0.25557905435562134.\tTime: 4.0\n",
      "MSE loss: 0.001623083371669054.\tMy loss: 0.25449278950691223.\tTime: 4.0\n",
      "\n",
      "\tAvarage lost in the last 20: 0.0016635846113786101\n",
      "\n",
      "does he felt somehow of but pretty .\n",
      ".\n",
      "trees it every pressure to but put still the along into just well kept instead break out .\n",
      "in turning out taken the along .\n",
      "one .\n",
      "kept before their moving .\n",
      "even out but while the along turning on on on on the along ##################################\n",
      "## EPOCH 221\n",
      "##################################\n",
      "MSE loss: 0.0016229980392381549.\tMy loss: 0.2544543445110321.\tTime: 5.0\n",
      "MSE loss: 0.0016057476168498397.\tMy loss: 0.2531685531139374.\tTime: 4.0\n",
      "MSE loss: 0.001609253347851336.\tMy loss: 0.2532224655151367.\tTime: 4.0\n",
      "MSE loss: 0.0016007261583581567.\tMy loss: 0.2529994249343872.\tTime: 5.0\n",
      "MSE loss: 0.0015957587165758014.\tMy loss: 0.25234416127204895.\tTime: 7.0\n",
      "MSE loss: 0.001608421211130917.\tMy loss: 0.25317293405532837.\tTime: 5.0\n",
      "MSE loss: 0.0016145877307280898.\tMy loss: 0.2537211775779724.\tTime: 6.0\n",
      "MSE loss: 0.001600910327397287.\tMy loss: 0.2522607743740082.\tTime: 5.0\n",
      "MSE loss: 0.0015991907566785812.\tMy loss: 0.2523498237133026.\tTime: 6.0\n",
      "MSE loss: 0.0015913019888103008.\tMy loss: 0.25175222754478455.\tTime: 6.0\n",
      "MSE loss: 0.0015856880927458405.\tMy loss: 0.25132644176483154.\tTime: 5.0\n",
      "MSE loss: 0.0015819082036614418.\tMy loss: 0.25066742300987244.\tTime: 5.0\n",
      "MSE loss: 0.0015836666570976377.\tMy loss: 0.2508834898471832.\tTime: 4.0\n",
      "MSE loss: 0.0015742508694529533.\tMy loss: 0.25014784932136536.\tTime: 6.0\n",
      "MSE loss: 0.001574065419845283.\tMy loss: 0.2504254877567291.\tTime: 4.0\n",
      "MSE loss: 0.0015788128366693854.\tMy loss: 0.25036585330963135.\tTime: 4.0\n",
      "MSE loss: 0.0015677106566727161.\tMy loss: 0.24972927570343018.\tTime: 5.0\n",
      "MSE loss: 0.0015651348512619734.\tMy loss: 0.24939224123954773.\tTime: 5.0\n",
      "MSE loss: 0.001552524627186358.\tMy loss: 0.24843281507492065.\tTime: 5.0\n",
      "MSE loss: 0.0015579044120386243.\tMy loss: 0.24864399433135986.\tTime: 5.0\n",
      "\n",
      "\tAvarage lost in the last 20: 0.0015885280445218086\n",
      "\n",
      "does he had torment it but father had never apparently hands .\n",
      "so when was looked took everyone in but ever well but so .\n",
      "though nevertheless always although small well of famous is as music soft kind instance .\n",
      "but he laughed was seemingly as , when he laughed was but they loudly ##################################\n",
      "## EPOCH 241\n",
      "##################################\n",
      "MSE loss: 0.0015604111831635237.\tMy loss: 0.24864286184310913.\tTime: 7.0\n",
      "MSE loss: 0.0015463816234841943.\tMy loss: 0.24751432240009308.\tTime: 6.0\n",
      "MSE loss: 0.0015406912425532937.\tMy loss: 0.24726396799087524.\tTime: 6.0\n",
      "MSE loss: 0.0015494192484766245.\tMy loss: 0.2475779801607132.\tTime: 6.0\n",
      "MSE loss: 0.001535771880298853.\tMy loss: 0.24628326296806335.\tTime: 5.0\n",
      "MSE loss: 0.0015250272117555141.\tMy loss: 0.2455706149339676.\tTime: 5.0\n",
      "MSE loss: 0.0015354944625869393.\tMy loss: 0.24647505581378937.\tTime: 6.0\n",
      "MSE loss: 0.0015376363880932331.\tMy loss: 0.24658507108688354.\tTime: 4.0\n",
      "MSE loss: 0.001531807123683393.\tMy loss: 0.24604350328445435.\tTime: 5.0\n",
      "MSE loss: 0.0015235465252771974.\tMy loss: 0.24540242552757263.\tTime: 5.0\n",
      "MSE loss: 0.001542716403491795.\tMy loss: 0.24642112851142883.\tTime: 4.0\n",
      "MSE loss: 0.0015269207069650292.\tMy loss: 0.24535878002643585.\tTime: 6.0\n",
      "MSE loss: 0.0015190899139270186.\tMy loss: 0.24508050084114075.\tTime: 5.0\n",
      "MSE loss: 0.001528952969238162.\tMy loss: 0.2457408905029297.\tTime: 5.0\n",
      "MSE loss: 0.0015245331451296806.\tMy loss: 0.2451000213623047.\tTime: 4.0\n",
      "MSE loss: 0.0015259997453540564.\tMy loss: 0.24525190889835358.\tTime: 6.0\n",
      "MSE loss: 0.0015266062691807747.\tMy loss: 0.2452201396226883.\tTime: 5.0\n",
      "MSE loss: 0.0014985862653702497.\tMy loss: 0.24293677508831024.\tTime: 4.0\n",
      "MSE loss: 0.0015090620145201683.\tMy loss: 0.2436838299036026.\tTime: 4.0\n",
      "MSE loss: 0.0015037081902846694.\tMy loss: 0.24328941106796265.\tTime: 4.0\n",
      "\n",
      "\tAvarage lost in the last 20: 0.0015296180499717593\n",
      "\n",
      "does he had clutched as same same thought friend thought .\n",
      "always this but any this always but not what what never not something was same as as a celebration singing .\n",
      "him he while instead .\n",
      "small out that the wild green they there but rest day came to although coming way this ##################################\n",
      "## EPOCH 261\n",
      "##################################\n",
      "MSE loss: 0.001495121861808002.\tMy loss: 0.2424018234014511.\tTime: 7.0\n",
      "MSE loss: 0.0015005633467808366.\tMy loss: 0.24287089705467224.\tTime: 6.0\n",
      "MSE loss: 0.0015020613791421056.\tMy loss: 0.24313034117221832.\tTime: 6.0\n",
      "MSE loss: 0.0014996487880125642.\tMy loss: 0.2427918016910553.\tTime: 4.0\n",
      "MSE loss: 0.0014831324806436896.\tMy loss: 0.24158607423305511.\tTime: 4.0\n",
      "MSE loss: 0.0014858260983601213.\tMy loss: 0.2415420413017273.\tTime: 4.0\n",
      "MSE loss: 0.001484334236010909.\tMy loss: 0.24147042632102966.\tTime: 4.0\n",
      "MSE loss: 0.0014858274953439832.\tMy loss: 0.24141190946102142.\tTime: 4.0\n",
      "MSE loss: 0.001477197976782918.\tMy loss: 0.24069781601428986.\tTime: 5.0\n",
      "MSE loss: 0.0014739425387233496.\tMy loss: 0.2403423935174942.\tTime: 5.0\n",
      "MSE loss: 0.0014733714051544666.\tMy loss: 0.2404199242591858.\tTime: 4.0\n",
      "MSE loss: 0.0014710196992382407.\tMy loss: 0.2402014136314392.\tTime: 4.0\n",
      "MSE loss: 0.0014779837802052498.\tMy loss: 0.2406567633152008.\tTime: 4.0\n",
      "MSE loss: 0.0014584481250494719.\tMy loss: 0.23900239169597626.\tTime: 4.0\n",
      "MSE loss: 0.0014662459725514054.\tMy loss: 0.2395825982093811.\tTime: 4.0\n",
      "MSE loss: 0.0014618940185755491.\tMy loss: 0.2392881214618683.\tTime: 4.0\n",
      "MSE loss: 0.0014597411500290036.\tMy loss: 0.23928432166576385.\tTime: 4.0\n",
      "MSE loss: 0.001455347635783255.\tMy loss: 0.23847836256027222.\tTime: 4.0\n",
      "MSE loss: 0.0014386872062459588.\tMy loss: 0.23729653656482697.\tTime: 4.0\n",
      "MSE loss: 0.0014433691976591945.\tMy loss: 0.23786605894565582.\tTime: 4.0\n",
      "\n",
      "\tAvarage lost in the last 20: 0.001474688295274973\n",
      "\n",
      "does he knew me as nothing even but i want suddenly when in so cut .\n",
      "coming off while out out up in one beyond near the east .\n",
      "on along and stone and walls and .\n",
      "far into but left where well yellow dark light light and .\n",
      "and atop of glass blue ##################################\n",
      "## EPOCH 281\n",
      "##################################\n",
      "MSE loss: 0.0014471247559413314.\tMy loss: 0.2376628965139389.\tTime: 5.0\n",
      "MSE loss: 0.0014505044091492891.\tMy loss: 0.2377084493637085.\tTime: 4.0\n",
      "MSE loss: 0.0014432395109906793.\tMy loss: 0.2371802181005478.\tTime: 4.0\n",
      "MSE loss: 0.0014422612730413675.\tMy loss: 0.237209290266037.\tTime: 4.0\n",
      "MSE loss: 0.0014421306550502777.\tMy loss: 0.23718410730361938.\tTime: 4.0\n",
      "MSE loss: 0.0014469192828983068.\tMy loss: 0.23749573528766632.\tTime: 4.0\n",
      "MSE loss: 0.001429378753527999.\tMy loss: 0.23613253235816956.\tTime: 8.0\n",
      "MSE loss: 0.0014265085337683558.\tMy loss: 0.23597876727581024.\tTime: 7.0\n",
      "MSE loss: 0.001432244200259447.\tMy loss: 0.2362963855266571.\tTime: 4.0\n",
      "MSE loss: 0.0014297267189249396.\tMy loss: 0.23598524928092957.\tTime: 4.0\n",
      "MSE loss: 0.00141743419226259.\tMy loss: 0.2352466881275177.\tTime: 4.0\n",
      "MSE loss: 0.0014354308368638158.\tMy loss: 0.23660163581371307.\tTime: 8.0\n",
      "MSE loss: 0.001424590707756579.\tMy loss: 0.23558543622493744.\tTime: 5.0\n",
      "MSE loss: 0.0014180109137669206.\tMy loss: 0.23476244509220123.\tTime: 5.0\n",
      "MSE loss: 0.0014094070065766573.\tMy loss: 0.23435722291469574.\tTime: 4.0\n",
      "MSE loss: 0.0014135204255580902.\tMy loss: 0.23426604270935059.\tTime: 5.0\n",
      "MSE loss: 0.0014206520281732082.\tMy loss: 0.23485565185546875.\tTime: 5.0\n",
      "MSE loss: 0.0013984530232846737.\tMy loss: 0.2331402450799942.\tTime: 5.0\n",
      "MSE loss: 0.0014021563110873103.\tMy loss: 0.23324963450431824.\tTime: 4.0\n",
      "MSE loss: 0.0014170800568535924.\tMy loss: 0.23441095650196075.\tTime: 5.0\n",
      "\n",
      "\tAvarage lost in the last 20: 0.0014273386914283037\n",
      "\n",
      "does he had crying .\n",
      "but when as he but never was mythology .\n",
      "the part another where around the winding in in one see but turned after him .\n",
      "but trying and imagined just .\n",
      "so on even between the along which the well teeth but while .\n",
      "came came put while but ##################################\n",
      "## EPOCH 301\n",
      "##################################\n",
      "MSE loss: 0.0014052556362003088.\tMy loss: 0.23362646996974945.\tTime: 5.0\n",
      "MSE loss: 0.0014062379486858845.\tMy loss: 0.23355305194854736.\tTime: 5.0\n",
      "MSE loss: 0.0013986832927912474.\tMy loss: 0.23308207094669342.\tTime: 4.0\n",
      "MSE loss: 0.0013912335271015763.\tMy loss: 0.23239988088607788.\tTime: 5.0\n",
      "MSE loss: 0.0013986723497509956.\tMy loss: 0.23296061158180237.\tTime: 5.0\n",
      "MSE loss: 0.0013927293475717306.\tMy loss: 0.2326183021068573.\tTime: 5.0\n",
      "MSE loss: 0.0013908111723139882.\tMy loss: 0.23197725415229797.\tTime: 5.0\n",
      "MSE loss: 0.0013905117521062493.\tMy loss: 0.23221023380756378.\tTime: 4.0\n",
      "MSE loss: 0.0013843645574524999.\tMy loss: 0.23177438974380493.\tTime: 4.0\n",
      "MSE loss: 0.0013880241895094514.\tMy loss: 0.23170951008796692.\tTime: 4.0\n",
      "MSE loss: 0.0013864575885236263.\tMy loss: 0.23171891272068024.\tTime: 4.0\n",
      "MSE loss: 0.0013793286634609103.\tMy loss: 0.23109692335128784.\tTime: 4.0\n",
      "MSE loss: 0.001378755085170269.\tMy loss: 0.23098023235797882.\tTime: 5.0\n",
      "MSE loss: 0.0013703005388379097.\tMy loss: 0.2303486168384552.\tTime: 5.0\n",
      "MSE loss: 0.001375440857373178.\tMy loss: 0.23080319166183472.\tTime: 5.0\n",
      "MSE loss: 0.0013653988717123866.\tMy loss: 0.22975844144821167.\tTime: 4.0\n",
      "MSE loss: 0.0013686052989214659.\tMy loss: 0.22983583807945251.\tTime: 4.0\n",
      "MSE loss: 0.0013840077444911003.\tMy loss: 0.2308380901813507.\tTime: 4.0\n",
      "MSE loss: 0.001365997246466577.\tMy loss: 0.22946539521217346.\tTime: 5.0\n",
      "MSE loss: 0.001366327633149922.\tMy loss: 0.22988663613796234.\tTime: 5.0\n",
      "\n",
      "\tAvarage lost in the last 20: 0.0013843572232872248\n",
      "\n",
      "does he had trembled in suddenly .\n",
      "him turned threatening to hey for but while when n't you not they had just every something .\n",
      "so i knowing .\n",
      "something always me know a beast .\n",
      "even so i one one still alive to .\n",
      "but laughing had seen this though disappeared in but ##################################\n",
      "## EPOCH 321\n",
      "##################################\n",
      "MSE loss: 0.001366809825412929.\tMy loss: 0.22984452545642853.\tTime: 5.0\n",
      "MSE loss: 0.0013684395235031843.\tMy loss: 0.2296808809041977.\tTime: 5.0\n",
      "MSE loss: 0.001363897928968072.\tMy loss: 0.22948333621025085.\tTime: 5.0\n",
      "MSE loss: 0.0013735073152929544.\tMy loss: 0.2300812005996704.\tTime: 4.0\n",
      "MSE loss: 0.0013652234338223934.\tMy loss: 0.2291988730430603.\tTime: 5.0\n",
      "MSE loss: 0.0013658850220963359.\tMy loss: 0.22963657975196838.\tTime: 5.0\n",
      "MSE loss: 0.0013511404395103455.\tMy loss: 0.22828377783298492.\tTime: 5.0\n",
      "MSE loss: 0.0013509991113096476.\tMy loss: 0.22821401059627533.\tTime: 5.0\n",
      "MSE loss: 0.0013573000906035304.\tMy loss: 0.22871257364749908.\tTime: 5.0\n",
      "MSE loss: 0.0013379589654505253.\tMy loss: 0.2271367758512497.\tTime: 4.0\n",
      "MSE loss: 0.00134231464471668.\tMy loss: 0.22698065638542175.\tTime: 4.0\n",
      "MSE loss: 0.0013377668801695108.\tMy loss: 0.22712554037570953.\tTime: 4.0\n",
      "MSE loss: 0.0013375217095017433.\tMy loss: 0.22671820223331451.\tTime: 5.0\n",
      "MSE loss: 0.0013433349085971713.\tMy loss: 0.22700907289981842.\tTime: 5.0\n",
      "MSE loss: 0.001334754633717239.\tMy loss: 0.2261386513710022.\tTime: 4.0\n",
      "MSE loss: 0.001330072176642716.\tMy loss: 0.22588664293289185.\tTime: 5.0\n",
      "MSE loss: 0.0013409149833023548.\tMy loss: 0.2273092120885849.\tTime: 5.0\n",
      "MSE loss: 0.001333963475190103.\tMy loss: 0.22620050609111786.\tTime: 5.0\n",
      "MSE loss: 0.0013354825787246227.\tMy loss: 0.22629505395889282.\tTime: 5.0\n",
      "MSE loss: 0.0013272430514916778.\tMy loss: 0.22568470239639282.\tTime: 5.0\n",
      "\n",
      "\tAvarage lost in the last 20: 0.0013482265640050173\n",
      "\n",
      "does he had kissed in disguise own own one came as as one one this a dark new .\n",
      "sight away but around along .\n",
      "the outer on along the the forest and into , even the view which , included well one came as a delight black black as but they thought he ##################################\n",
      "## EPOCH 341\n",
      "##################################\n",
      "MSE loss: 0.0013320319121703506.\tMy loss: 0.22605331242084503.\tTime: 5.0\n",
      "MSE loss: 0.0013168479781597853.\tMy loss: 0.22493399679660797.\tTime: 4.0\n",
      "MSE loss: 0.001324865035712719.\tMy loss: 0.22489993274211884.\tTime: 5.0\n",
      "MSE loss: 0.001335714478045702.\tMy loss: 0.2260037660598755.\tTime: 4.0\n",
      "MSE loss: 0.001324852230027318.\tMy loss: 0.22512249648571014.\tTime: 4.0\n",
      "MSE loss: 0.0013098876224830747.\tMy loss: 0.22386446595191956.\tTime: 5.0\n",
      "MSE loss: 0.001319300034083426.\tMy loss: 0.22464342415332794.\tTime: 4.0\n",
      "MSE loss: 0.0013253436191007495.\tMy loss: 0.22471635043621063.\tTime: 5.0\n",
      "MSE loss: 0.001324231387116015.\tMy loss: 0.2248745858669281.\tTime: 4.0\n",
      "MSE loss: 0.0013028326211497188.\tMy loss: 0.22307059168815613.\tTime: 4.0\n",
      "MSE loss: 0.0013072640867903829.\tMy loss: 0.22350895404815674.\tTime: 5.0\n",
      "MSE loss: 0.001307541853748262.\tMy loss: 0.22354604303836823.\tTime: 4.0\n",
      "MSE loss: 0.001289915875531733.\tMy loss: 0.22207427024841309.\tTime: 4.0\n",
      "MSE loss: 0.0012968279188498855.\tMy loss: 0.22259333729743958.\tTime: 4.0\n",
      "MSE loss: 0.001280520111322403.\tMy loss: 0.2210465371608734.\tTime: 4.0\n",
      "MSE loss: 0.0012979875318706036.\tMy loss: 0.22246100008487701.\tTime: 5.0\n",
      "MSE loss: 0.001295453286729753.\tMy loss: 0.22241352498531342.\tTime: 4.0\n",
      "MSE loss: 0.001301974873058498.\tMy loss: 0.2228916585445404.\tTime: 4.0\n",
      "MSE loss: 0.001303988741710782.\tMy loss: 0.22303515672683716.\tTime: 5.0\n",
      "MSE loss: 0.001295770169235766.\tMy loss: 0.22266364097595215.\tTime: 5.0\n",
      "\n",
      "\tAvarage lost in the last 20: 0.0013096575858071446\n",
      "\n",
      "does he had kissed .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "then but him had turned came but he had him a furry on the edge on the within one one just even came before up on with out in the edge , well but but to the selected in hanging while came but though knew a one some .\n",
      "##################################\n",
      "## EPOCH 361\n",
      "##################################\n",
      "MSE loss: 0.0012958961306139827.\tMy loss: 0.22213172912597656.\tTime: 6.0\n",
      "MSE loss: 0.0012897794367745519.\tMy loss: 0.22204962372779846.\tTime: 4.0\n",
      "MSE loss: 0.0012874999083578587.\tMy loss: 0.2216554582118988.\tTime: 4.0\n",
      "MSE loss: 0.0013048346154391766.\tMy loss: 0.22297048568725586.\tTime: 4.0\n",
      "MSE loss: 0.0012880921130999923.\tMy loss: 0.22182683646678925.\tTime: 4.0\n",
      "MSE loss: 0.00128476123791188.\tMy loss: 0.22128596901893616.\tTime: 4.0\n",
      "MSE loss: 0.0012942058965563774.\tMy loss: 0.22199209034442902.\tTime: 4.0\n",
      "MSE loss: 0.0012852989602833986.\tMy loss: 0.22146955132484436.\tTime: 4.0\n",
      "MSE loss: 0.0012720492668449879.\tMy loss: 0.22007249295711517.\tTime: 4.0\n",
      "MSE loss: 0.0012607681564986706.\tMy loss: 0.21887727081775665.\tTime: 4.0\n",
      "MSE loss: 0.0012728039873763919.\tMy loss: 0.21967732906341553.\tTime: 4.0\n",
      "MSE loss: 0.0012695639161393046.\tMy loss: 0.2196994423866272.\tTime: 4.0\n",
      "MSE loss: 0.0012680632062256336.\tMy loss: 0.2194165587425232.\tTime: 4.0\n",
      "MSE loss: 0.0012738885125145316.\tMy loss: 0.21974055469036102.\tTime: 4.0\n",
      "MSE loss: 0.001277955831028521.\tMy loss: 0.21995413303375244.\tTime: 4.0\n",
      "MSE loss: 0.001271075219847262.\tMy loss: 0.21976804733276367.\tTime: 4.0\n",
      "MSE loss: 0.0012841667048633099.\tMy loss: 0.22080758213996887.\tTime: 4.0\n",
      "MSE loss: 0.001269437954761088.\tMy loss: 0.21961621940135956.\tTime: 4.0\n",
      "MSE loss: 0.0012634742306545377.\tMy loss: 0.21896012127399445.\tTime: 4.0\n",
      "MSE loss: 0.0012629956472665071.\tMy loss: 0.21891716122627258.\tTime: 4.0\n",
      "\n",
      "\tAvarage lost in the last 20: 0.0012788305757567286\n",
      "\n",
      "does he had spades as but once though never but got whacked knot over apparently hand bouncing in the place but turning as turning .\n",
      ".\n",
      "the spiral the along on one edge of southward from the the sometimes of of legolas little on one one but once later making in their also unsheathed ##################################\n",
      "## EPOCH 381\n",
      "##################################\n",
      "MSE loss: 0.0012691352749243379.\tMy loss: 0.2194899618625641.\tTime: 5.0\n",
      "MSE loss: 0.0012630169512704015.\tMy loss: 0.21900364756584167.\tTime: 4.0\n",
      "MSE loss: 0.0012558104936033487.\tMy loss: 0.21804139018058777.\tTime: 4.0\n",
      "MSE loss: 0.0012557868612930179.\tMy loss: 0.2182084321975708.\tTime: 4.0\n",
      "MSE loss: 0.0012598730390891433.\tMy loss: 0.2186424285173416.\tTime: 4.0\n",
      "MSE loss: 0.0012526478385552764.\tMy loss: 0.21773241460323334.\tTime: 4.0\n",
      "MSE loss: 0.0012501102173700929.\tMy loss: 0.2175433337688446.\tTime: 6.0\n",
      "MSE loss: 0.0012577661545947194.\tMy loss: 0.2185029834508896.\tTime: 4.0\n",
      "MSE loss: 0.0012490723747760057.\tMy loss: 0.2176104485988617.\tTime: 4.0\n",
      "MSE loss: 0.0012468259083107114.\tMy loss: 0.21724607050418854.\tTime: 4.0\n",
      "MSE loss: 0.0012472330126911402.\tMy loss: 0.21722473204135895.\tTime: 4.0\n",
      "MSE loss: 0.0012468009954318404.\tMy loss: 0.21696236729621887.\tTime: 4.0\n",
      "MSE loss: 0.0012343742419034243.\tMy loss: 0.21600563824176788.\tTime: 4.0\n",
      "MSE loss: 0.001242800266481936.\tMy loss: 0.21684256196022034.\tTime: 4.0\n",
      "MSE loss: 0.001251516747288406.\tMy loss: 0.21754147112369537.\tTime: 4.0\n",
      "MSE loss: 0.001237582415342331.\tMy loss: 0.2163517028093338.\tTime: 4.0\n",
      "MSE loss: 0.00122899713460356.\tMy loss: 0.2152205854654312.\tTime: 4.0\n",
      "MSE loss: 0.0012384544825181365.\tMy loss: 0.21625302731990814.\tTime: 4.0\n",
      "MSE loss: 0.0012293432373553514.\tMy loss: 0.21521353721618652.\tTime: 4.0\n",
      "MSE loss: 0.0012302055256441236.\tMy loss: 0.21540017426013947.\tTime: 5.0\n",
      "\n",
      "\tAvarage lost in the last 20: 0.001247367705218494\n",
      "\n",
      "does he had stroked .\n",
      "when know he come ! simply believing but far it than much stretched back north while awkwardly back down the the one which as one look as well too so .\n",
      "of a shaped red of rare small look those seeing come advice whose came and one .\n",
      "but ##################################\n",
      "## EPOCH 401\n",
      "##################################\n",
      "MSE loss: 0.0012451766524463892.\tMy loss: 0.21639710664749146.\tTime: 5.0\n",
      "MSE loss: 0.0012427158653736115.\tMy loss: 0.2164076864719391.\tTime: 4.0\n",
      "MSE loss: 0.0012391264317557216.\tMy loss: 0.21642138063907623.\tTime: 5.0\n",
      "MSE loss: 0.0012348843738436699.\tMy loss: 0.21564196050167084.\tTime: 4.0\n",
      "MSE loss: 0.0012277110945433378.\tMy loss: 0.21501927077770233.\tTime: 4.0\n",
      "MSE loss: 0.001227891887538135.\tMy loss: 0.21517817676067352.\tTime: 4.0\n",
      "MSE loss: 0.0012249037390574813.\tMy loss: 0.2146843820810318.\tTime: 4.0\n",
      "MSE loss: 0.0012164555955678225.\tMy loss: 0.21423693001270294.\tTime: 4.0\n",
      "MSE loss: 0.0012174742296338081.\tMy loss: 0.21400204300880432.\tTime: 4.0\n",
      "MSE loss: 0.0012185715604573488.\tMy loss: 0.21422894299030304.\tTime: 4.0\n",
      "MSE loss: 0.0012248033890500665.\tMy loss: 0.2144143134355545.\tTime: 4.0\n",
      "MSE loss: 0.0012291820021346211.\tMy loss: 0.21523091197013855.\tTime: 4.0\n",
      "MSE loss: 0.0012268363498151302.\tMy loss: 0.21481123566627502.\tTime: 4.0\n",
      "MSE loss: 0.0012152560520917177.\tMy loss: 0.21405532956123352.\tTime: 4.0\n",
      "MSE loss: 0.0012095024576410651.\tMy loss: 0.21312575042247772.\tTime: 4.0\n",
      "MSE loss: 0.0012156489538028836.\tMy loss: 0.21363559365272522.\tTime: 4.0\n",
      "MSE loss: 0.0012080701999366283.\tMy loss: 0.21309137344360352.\tTime: 4.0\n",
      "MSE loss: 0.0012067242059856653.\tMy loss: 0.2129504531621933.\tTime: 5.0\n",
      "MSE loss: 0.0012171370908617973.\tMy loss: 0.21393820643424988.\tTime: 5.0\n",
      "MSE loss: 0.0012075684498995543.\tMy loss: 0.2132507562637329.\tTime: 4.0\n",
      "\n",
      "\tAvarage lost in the last 20: 0.0012227820698171854\n",
      "\n",
      "does he had kissed .\n",
      "once never being came him .\n",
      "but this as on even way came just with out from circle along where while around , still suddenly but while in 've when to the given this to tell of god name in still but the enemy of the little .\n",
      "saw ##################################\n",
      "## EPOCH 421\n",
      "##################################\n",
      "MSE loss: 0.001211875700391829.\tMy loss: 0.21304714679718018.\tTime: 6.0\n",
      "MSE loss: 0.0012069933582097292.\tMy loss: 0.21282772719860077.\tTime: 4.0\n",
      "MSE loss: 0.0012111488031223416.\tMy loss: 0.21306245028972626.\tTime: 5.0\n",
      "MSE loss: 0.0012170670088380575.\tMy loss: 0.21343527734279633.\tTime: 4.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-954551620262>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mb_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# Iterate batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_sample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[1;31m# Extract batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mbatch_onehot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoded_onehot'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-7d634ca49671>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;31m# Transform (if defined)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-7d634ca49671>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, sample)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;31m# Convert one hot encoded text to pytorch tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0mencoded_onehot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoded_onehot'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'encoded_onehot'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mencoded_onehot\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#%% Check device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print('Selected device:', device)\n",
    "\n",
    "#%% Initialize network\n",
    "net = Network(input_size=50, \n",
    "              hidden_units=256, \n",
    "              layers_num=2, \n",
    "              dropout_prob=0.3)\n",
    "net.to(device)\n",
    "validation = validation.to(device).float()\n",
    "#%% Train network\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.003)\n",
    "# Define loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "loss = []\n",
    "ver = 20\n",
    "# Start training\n",
    "for epoch in range(600):\n",
    "    start = time.time()\n",
    "    if epoch%ver == 0:\n",
    "        if len(loss) > ver:\n",
    "            print('\\n\\tAvarage lost in the last {}: {}\\n'.format(ver, np.mean(loss[epoch-ver:epoch])))\n",
    "            testing(net, emb, dataset, 'does he', length = 50)\n",
    "        print('##################################')\n",
    "        print('## EPOCH %d' % (epoch + 1))\n",
    "        print('##################################')\n",
    "    b_losses, m_losses = [], []\n",
    "    # Iterate batches\n",
    "    for batch_sample in dataloader:\n",
    "        # Extract batch\n",
    "        batch_onehot = batch_sample['encoded_onehot'].to(device)\n",
    "        \n",
    "        # Update network\n",
    "        batch_loss, my_loss, out, y_true = train_batch(net, batch_onehot, loss_fn, optimizer)    \n",
    "        \n",
    "        b_losses.append(batch_loss)\n",
    "        m_losses.append(my_loss)\n",
    "        \n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            y_validation = validation[:, -1, :]\n",
    "            #labels_numbers = labels_onehot.argmax(dim=1)\n",
    "            # Remove the labels from the input tensor\n",
    "            val_input = validation[:, :-1, :]\n",
    "            validation_pred, _ = net(val_input)\n",
    "            #net_out  = nn.functional.softmax(net_out, dim=1)\n",
    "            #print(net_out.shape)\n",
    "            ### Update network\n",
    "            # Evaluate loss only for last output\n",
    "            loss_val = loss_fn(validation_pred[:, -1, :], y_validation)\n",
    "        \"\"\"  \n",
    "        \n",
    "        #if epoch%20 == 0:\n",
    "            #print('\\t Training loss (single batch):', batch_loss)\n",
    "    loss.append(torch.mean(torch.tensor(b_losses)))\n",
    "    #if epoch%20 == 0:\n",
    "    print('MSE loss: {}.'.format(torch.mean(torch.tensor(b_losses))), end='\\t')\n",
    "    print('My loss: {}.'.format(torch.mean(torch.tensor(m_losses))), end='\\t')\n",
    "    #print('Validation loss: {}'.format(loss_val.data), end='\\t')\n",
    "    print('Time: {}'.format(np.round(time.time()-start), 4))\n",
    "print(loss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(batch_out):\n",
    "    batch_size, nfeatures = batch_out.shape[0], batch_out.shape[-1] \n",
    "    batch = torch.reshape(batch_out, (-1, nfeatures))\n",
    "    batch = batch/torch.norm(batch, dim=1).view(batch.shape[0], 1)\n",
    "    return torch.reshape(batch_out, (batch_size, -1, nfeatures))\n",
    "\n",
    "out = normalize(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he went up and down the shire and man only but he few one black dwarves once came but he came suddenly . once back out from came long just when the the south before the on . . just on one over as wide long backs blue surface one walking when standing but when . just back . splayed back bouncing legs out came away the shore on on where one came where when accidentally , still as on inside to across across along the the the the part one day . they dragon put turn . coming but on come on the with small of were than "
     ]
    }
   ],
   "source": [
    "state = 'he went up and down the shire'\n",
    "X = emb.values\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Encode seed\n",
    "    seed_encoded = encode_text(dataset.char_to_number, state.lower(), emb)\n",
    "    # One hot matrix\n",
    "    #seed_onehot = create_one_hot_matrix(seed_encoded, 47)\n",
    "    # To tensor\n",
    "    seed_onehot = torch.tensor(seed_encoded).float()\n",
    "    #print(seed_onehot)\n",
    "    # Add batch axis\n",
    "    seed_onehot = seed_onehot.unsqueeze(0)\n",
    "    # Forward pass\n",
    "    seed_onehot = seed_onehot.to(device)\n",
    "    net_out, net_state = net(seed_onehot)\n",
    "    # Get the most probable last output index\n",
    "    next_word_encoded = net_out[:, -1, :]\n",
    "    closest_value = np.linalg.norm((X - next_word_encoded.to('cpu').numpy()[0]), axis = 1)\n",
    "    closest_word = emb.index[np.argmin(closest_value)]\n",
    "    state += ' ' + closest_word\n",
    "    # Print the seed letters\n",
    "    print(state, end=' ', flush=True)\n",
    "#%% Generate sonnet\n",
    "new_line_count = 0\n",
    "tot_char_count = 0\n",
    "while True:\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        # The new network input is the one hot encoding of the last chosen letter\n",
    "        net_input = encode_text(dataset.char_to_number, state.lower(), emb)\n",
    "        net_input = torch.reshape(torch.tensor(net_input).float(), (1, -1, 50))\n",
    "        #print(net_input.shape)\n",
    "        #net_input = net_input.unsqueeze(0)\n",
    "        # Forward pass\n",
    "        net_input = net_input.to(device)\n",
    "        net_out, net_state = net(net_input, net_state)\n",
    "        \n",
    "        # Get the most probable letter index\n",
    "        closest_value = np.linalg.norm((X - net_out[:, -1, :].to('cpu').numpy()[0]), axis = 1)\n",
    "        closest_word = emb.index[np.argmin(closest_value)]\n",
    "        state += ' ' + closest_word\n",
    "        print(closest_word, end=' ', flush=True)\n",
    "        # Count total letters\n",
    "        tot_char_count += 1\n",
    "        # Count new lines\n",
    "        # Break if 14 lines or 2000 letters\n",
    "        if tot_char_count > 100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fgrim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Network. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(net, 'model_at_03_dec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he went up and down the shire and friend they .\n",
      "that raven but resting front came smiling away when one away .\n",
      "even but while they soon come but even but when they but he when away when just as 've suddenly .\n",
      "seemed coming , some they many tried still on way of the one four rest "
     ]
    }
   ],
   "source": [
    "net1 = torch.load('model_at_03_dec')\n",
    "testing(net1, emb, dataset, 'he went up and down the shire', length = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'had', 'not', 'forgotten', 'the', 'look', 'of']\n",
      "['was', 'very', 'anxious', 'to', 'find', 'some', 'sign']\n",
      "['when', 'autumn', 'came', ',', 'he', 'knew', 'that']\n",
      "['i', 'imagine', 'what', 'spring', 'would', 'look', 'like']\n",
      "[\"'\", 'when', 'he', 'heard', 'what', 'frodo', 'had']\n",
      "torch.Size([5, 6, 50])\n",
      "##############\n",
      "had\twhen\n",
      "0.40604567527770996 \t 0.3945769998408468\n",
      "not\tturned\n",
      "0.8561952114105225 \t 0.43178995707111495\n",
      "forgotten\toccasion\n",
      "1.2295607328414917 \t 0.8516743971424025\n",
      "the\tconfirm\n",
      "1.3213441371917725 \t 0.9896899455375435\n",
      "look\tground\n",
      "1.0786402225494385 \t 0.8247855164957774\n",
      "of\tof\n",
      "0.03832031413912773 \t 0.038320315528186946\n",
      "##############\n",
      "very\tstill\n",
      "0.5128685235977173 \t 0.32169898089498167\n",
      "anxious\tmade\n",
      "0.9829562902450562 \t 0.5219056313426617\n",
      "to\t,\n",
      "0.5812139511108398 \t 0.23428700202510533\n",
      "find\tpity\n",
      "0.887028694152832 \t 0.7011574674714168\n",
      "some\thim\n",
      "0.7747485041618347 \t 0.33100224672743744\n",
      "sign\tsign\n",
      "0.46773993968963623 \t 0.4677399386228759\n",
      "##############\n",
      "autumn\tbut\n",
      "1.002932071685791 \t 0.24991816051544585\n",
      "came\t.\n",
      "0.4280683696269989 \t 0.39288504674658825\n",
      ",\tyellow\n",
      "0.9168917536735535 \t 0.6704107408373069\n",
      "he\tsoon\n",
      "0.7037444710731506 \t 0.5304748545729866\n",
      "knew\tspinning\n",
      "1.101285696029663 \t 0.8018807748640195\n",
      "that\tthat\n",
      "0.18727773427963257 \t 0.18727771731507825\n",
      "##############\n",
      "imagine\tbut\n",
      "0.6668714880943298 \t 0.23117118669066802\n",
      "what\twhich\n",
      "0.604468584060669 \t 0.3043024858804103\n",
      "spring\t.\n",
      "0.8221362829208374 \t 0.3991544976118416\n",
      "would\tthe\n",
      "0.830382227897644 \t 0.6160571096797925\n",
      "look\twant\n",
      "0.5638960599899292 \t 0.4453959593854115\n",
      "like\tlike\n",
      "0.22327923774719238 \t 0.22327924045556086\n",
      "##############\n",
      "when\tasked\n",
      "0.553896963596344 \t 0.4042290826960784\n",
      "he\tsaid\n",
      "0.8315635323524475 \t 0.4172607832950673\n",
      "heard\tcarried\n",
      "0.9739494323730469 \t 0.6981076052453312\n",
      "what\t,\n",
      "0.9541437029838562 \t 0.5221373788245569\n",
      "frodo\tquietly\n",
      "1.2218464612960815 \t 0.557297164593142\n",
      "had\thad\n",
      "0.19167090952396393 \t 0.19167091636710687\n"
     ]
    }
   ],
   "source": [
    "for batch_sample in dataloader:\n",
    "    # Extract batch\n",
    "    batch_onehot = batch_sample['encoded_onehot'][0:5, :, :]\n",
    "    for i in range(5):\n",
    "        print(decode_text(dataset.emb, batch_onehot[i, :, :].numpy()))\n",
    "    \n",
    "    batch_onehot_dev = batch_onehot.to(device)\n",
    "    # Update network\n",
    "    batch_loss, my_loss, out, y_true = train_batch(net, batch_onehot_dev, loss_fn, optimizer)\n",
    "    print(out.shape)\n",
    "    \n",
    "    values = []\n",
    "    for i in range(5):\n",
    "        print('##############')\n",
    "        out[i, :, :].shape\n",
    "        out_word = out[i, :, :].cpu().detach().numpy()\n",
    "        for j in range(6):\n",
    "            values.append(out_word[j])\n",
    "            #Letter to predict\n",
    "            true_word = decode_text(dataset.emb, batch_onehot[i, :, :].numpy())[j+1]\n",
    "            true_value = np.linalg.norm(batch_onehot[i, :, :].numpy()[j+1] - out_word[j])\n",
    "            true_value = float(torch.norm(batch_onehot[i, j+1, :].cpu().float() - out[i, j, :].cpu()).data)\n",
    "            print(true_word, end = '\\t')\n",
    "            closest_value = np.argmin(np.linalg.norm((X - out_word[j]), axis = 1))\n",
    "            #print(np.linalg.norm((X - out_word[j]), axis = 1).shape)\n",
    "            closest_word = emb.index[closest_value]\n",
    "            print(closest_word)\n",
    "            print(true_value, '\\t', np.min((np.linalg.norm((X - out_word[j]), axis = 1))))\n",
    "            #print(np.min((np.linalg.norm((X - out_word[j]), axis = 1)))-true_value)\n",
    "            \n",
    "            #print(emb[emb.index == ''])\n",
    "    \n",
    "    b_losses.append(batch_loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(values))\n",
    "df = pd.DataFrame(values)\n",
    "df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.loc['banners', :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
